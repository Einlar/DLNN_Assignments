@article{pytorch_lightning,
  title={PyTorch Lightning},
  author={Falcon, WA},
  journal={GitHub. Note: https://github.com/PyTorchLightning/pytorch-lightning},
  volume={3},
  year={2019}
}


@online{optuna,
  title = {Optuna: {{A Next}}-Generation {{Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  date = {2019-07-25},
  url = {http://arxiv.org/abs/1907.10902},
  urldate = {2021-06-21},
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archiveprefix = {arXiv},
  eprint = {1907.10902},
  eprinttype = {arxiv},
  file = {C\:\\Users\\franc\\Zotero\\storage\\L432FLZ3\\Akiba et al. - 2019 - Optuna A Next-generation Hyperparameter Optimizat.pdf;C\:\\Users\\franc\\Zotero\\storage\\GRJ4AENP\\1907.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}




@online{vae,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  date = {2014-05-01},
  url = {http://arxiv.org/abs/1312.6114},
  urldate = {2021-06-21},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archiveprefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {C\:\\Users\\franc\\Zotero\\storage\\NFX4JYAM\\Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\franc\\Zotero\\storage\\MI64EP3Q\\1312.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}


