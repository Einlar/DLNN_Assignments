{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgHJFDwYvgHg"
   },
   "source": [
    "# Homework 2 - Unsupervised Deep Learning\n",
    "<center><b>Student</b>: Francesco Manzali (1234428)</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNENm7RHGFMd"
   },
   "source": [
    "## General overview\n",
    "In this homework you will learn how to implement and test neural network models for solving unsupervised problems. For simplicity and to allow continuity with the kind of data you have seen before, the homework will be based on images of handwritten digits (MNIST). However, you can optionally explore different image collections (e.g., [Caltech](http://www.vision.caltech.edu/Image_Datasets/Caltech101/) or [Cifar](https://www.cs.toronto.edu/~kriz/cifar.html)) or other datasets based on your interests. The basic tasks for the homework will require to test and analyze the convolutional autoencoder implemented during the Lab practice. If you prefer, you can opt for a fully-connected autoencoder, which should achieve similar performance considering the relatively small size of the MNIST images. As for the previous homework, you should explore the use of advanced optimizers and regularization methods. Learning hyperparameters should be tuned using appropriate search procedures, and final accuracy should be evaluated using a cross-validation setup. More advanced tasks will require the exploration of denoising and variational architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bZGjMokMvk2h"
   },
   "source": [
    "\n",
    "## Technical notes\n",
    "The homework should be implemented in Python using the PyTorch framework. The student can explore additional libraries and tools to implement the models; however, please make sure you understand the code you are writing because during the exam you might receive specific questions related to your implementation. The entire source code required to run the homework must be uploaded as a compressed archive in a Moodle section dedicated to the homework. If your code will be entirely included in a single Python notebook, just upload the notebook file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYZUBEMCvlpB"
   },
   "source": [
    "\n",
    "## Final report\n",
    "Along with the source code, you must separately upload a PDF file containing a brief report of your homework. The report should include a brief Introduction on which you explain the homework goals and the main implementation strategies you choose, a brief Method section where you describe your model architectures and hyperparameters, and a Result section where you present the simulation results. Total length must not exceed 6 pages, though you can include additional tables and figures in a final Appendix (optional).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RchPc7G6vmRB"
   },
   "source": [
    "\n",
    "## Grade\n",
    "The maximum grade for this homework will be **8 points**. Points will be assigned based on the correct implementation of the following items:\n",
    "*\t1 pt: implement and test (convolutional) autoencoder, reporting the trend of reconstruction loss and some examples of image reconstruction\n",
    "*\t1 pt: explore advanced optimizers and regularization methods \n",
    "*\t1 pt: optimize hyperparameters using grid/random search and cross-validation\n",
    "*\t1 pt: implement and test denoising (convolutional) autoencoder\n",
    "*\t1 pt: fine-tune the (convolutional) autoencoder using a supervised classification task (you can compare classification accuracy and learning speed with results achieved in homework 1)\n",
    "*\t1 pt: explore the latent space structure (e.g., PCA, t-SNE) and generate new samples from latent codes\n",
    "*\t2 pt: implement variational (convolutional) autoencoder or GAN\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYE6Cjhgvm3B"
   },
   "source": [
    "\n",
    "## Deadline\n",
    "The complete homework (source code + report) must be submitted through Moodle at least 10 days before the chosen exam date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "#Install all the required packages with the correct versions in the current environment.\n",
    "#Note: this notebook has been run with Python 3.9.0 on a 64-bit Windows 10 machine, with a AMD Ryzen 7 1700 8-core CPU, GTX 970 GPU, and 32GB of DDR4 RAM. \n",
    "!{sys.executable} -m pip install numpy~=1.20.1 pandas~=1.2.3 matplotlib~=3.3.4 hiplot~=0.1.24 scipy~=1.6.0 tqdm~=4.59.0 torch~=1.8.0 torchvision~=0.9.0 optuna~=2.7.0 pytorch_lightning~=1.3.4 torchmetrics~=0.3.2 torchsummaryX psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#Autoreload imported functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget \n",
    "#Or use %matplotlib notebook\n",
    "#I'm running jupyter inside of Visual Studio Code, so %matplotlib widget is needed for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl2latex import mpl2latex, latex_figsize\n",
    "from plotting import COLUMNWIDTH\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(filename='01_Conv_Autoencoder.log', encoding='utf-8', level=logging.INFO, filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.8.0\n",
      "Using \"cuda\" for training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "#Select device for training\n",
    "#device = \"cpu\" #For this very simple dataset it is actually faster\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") #Uncomment for GPU \n",
    "\n",
    "logging.info(f\"Using {device} for training\")\n",
    "print(f'Using \"{device}\" for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping \n",
    "from callbacks import MetricsCallback, LitProgressBar, ReconstructedImage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import MNISTDataModule\n",
    "\n",
    "#Note: the main difference from the previous homework is that samples here are not normalized to have mean=0 and std=1, but just to be in [0, 1]\n",
    "\n",
    "mnist = MNISTDataModule(batch_size=64) #No data augmentation (to make training faster)\n",
    "mnist.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dde161f78d640bc831d84f6b40e5910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.0, Max: 1.0\n"
     ]
    }
   ],
   "source": [
    "from plotting import show_sample\n",
    "\n",
    "sample, label = mnist.train_dataset[1]\n",
    "show_sample(sample, label)\n",
    "print(f\"Min: {torch.min(sample)}, Max: {torch.max(sample)}\") #Check the normalization (should be about [0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models\n",
    "## 2.1 Convolutional AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAutoEncoder(pl.LightningModule):\n",
    "    \"\"\"Convolutional AutoEncoder for MNIST\"\"\"\n",
    "    \n",
    "    #Output shape of convolutional layers:\n",
    "    #x_out = floor([x_in + 2 * x_pad - (x_ker - 1) - 1] / x_stride + 1)\n",
    "\n",
    "    def __init__(self, hyper_parameters : dict = None, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if hyper_parameters is None:\n",
    "            self.hyper_parameters = { #Default values\n",
    "                'optimizer' : 'Adam',\n",
    "                'encoded_space_dim' : 10,\n",
    "                'learning_rate' : 1e-3,\n",
    "                'dropout_rate' : 0\n",
    "            }\n",
    "            self.hyper_parameters.update(**kwargs)\n",
    "        else:\n",
    "            self.hyper_parameters = hyper_parameters\n",
    "\n",
    "        self.save_hyperparameters() #store hyper_parameters in checkpoints\n",
    "\n",
    "        self.dropout_rate = self.hyper_parameters['dropout_rate']\n",
    "        self.encoded_space_dim = self.hyper_parameters['encoded_space_dim']\n",
    "\n",
    "        #in = (1, 28, 28)\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1), #out = (8, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(self.dropout_rate),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1), #out = (16, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0), #out = (32, 3, 3)\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Flatten(start_dim=1),\n",
    "            nn.Linear(3 * 3 * 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(64, self.encoded_space_dim)\n",
    "        )\n",
    "\n",
    "        self.decoder_cnn = nn.Sequential( #Specular\n",
    "            nn.Linear(self.encoded_space_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(64, 3 * 3 * 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, output_padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout2d(self.dropout_rate),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid() #Force output to be in [0, 1] (valid pixel values)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x : \"torch.tensor\"):\n",
    "        embedding = self.encoder_cnn(x)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, _ = batch #ignore labels\n",
    "\n",
    "        internal_repr = self.encoder_cnn(x)\n",
    "        \n",
    "        x_hat = self.decoder_cnn(internal_repr)\n",
    "\n",
    "        loss = F.mse_loss(x_hat, x, reduction='mean')\n",
    "        self.log('train_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(optim, self.hyper_parameters['optimizer'])(self.parameters(), lr=self.hyper_parameters['learning_rate']) #, weight_decay=1e-5)\n",
    "        return optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, log_name='val_loss'):\n",
    "        x, _ = batch\n",
    "\n",
    "        internal_repr = self.encoder_cnn(x)\n",
    "        \n",
    "        x_hat = self.decoder_cnn(internal_repr)\n",
    "\n",
    "        loss = F.mse_loss(x_hat, x)\n",
    "        self.log(log_name, loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx, log_name='test_loss'):\n",
    "        \n",
    "        return self.validation_step(batch, batch_idx, log_name='test_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Testing Cross-Validation\n",
    "Is cross-validation necessary for autoencoders trained on MNIST? One way to find out is to use it, and see how much the losses computed on each fold are different. \n",
    "\n",
    "If they are really close, then any fixed train/val split would suffice, since the size of the validation dataset would be enough for estimating the generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41687a8e5e2041e1bd283e90beccb539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b192d5fdac3f4576b4a4893d282b409a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b336747c01034ef2be3a44957c7d2495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea9c29c5d9e460c93560e2f3c74bf23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Cross Validation Test---# #Takes about 2 min to complete.\n",
    "\n",
    "from data import MNIST_CV\n",
    "\n",
    "mnist_cv = MNIST_CV()\n",
    "\n",
    "fold_metrics = []\n",
    "\n",
    "for fold_idx, (train_dataloader, val_dataloader) in enumerate(mnist_cv.get_fold()):\n",
    "    logging.info(f\"Cross-validation - Fold {fold_idx} - Started\")\n",
    "    \n",
    "    bar = LitProgressBar() #Fixes a tqdm bug in Jupyter by disabling the validation progressbar during training\n",
    "    metrics_callback = MetricsCallback()\n",
    "    cnn_autoencoder = ConvAutoEncoder()\n",
    "    trainer = pl.Trainer(gpus=1, max_epochs=3, callbacks=[metrics_callback, bar], checkpoint_callback=False)\n",
    "    trainer.fit(cnn_autoencoder, train_dataloader, val_dataloader)\n",
    "\n",
    "    fold_metrics.append(metrics_callback.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average final loss: 0.0224, std: 0.0004\n"
     ]
    }
   ],
   "source": [
    "final_val_losses = [metric[-1]['val_loss'] for metric in fold_metrics]\n",
    "print(f\"Average final loss: {np.mean(final_val_losses):.4f}, std: {np.std(final_val_losses):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the losses are very similar in all cross-validation folds (the standard deviation is very low).\n",
    "This suggests that the dataset is sufficiently big that any of its k-parts (with $k=4$ here) suffices for reaching similar results. Thus, a full k-fold cross-validation can be neglected, and a fixed validation set may be used instead, reducing the amount of computation needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Training\n",
    "\n",
    "A CNN autoencoder is trained to compress MNIST samples to just two numbers. This allows to test if the model works, while also being able to plot directly the latent representations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "MODEL_SAVE_FOLDER = Path(\"SavedModels\")\n",
    "\n",
    "#---Helper functions for managing checkpoints---#\n",
    "def checkpoint_path(name : str):\n",
    "    \"\"\"Given a model `name`, return a path to its checkpoint\"\"\"\n",
    "    return MODEL_SAVE_FOLDER / Path(name + '.ckpt')\n",
    "\n",
    "def learn_curve_path(name : str):\n",
    "    \"\"\"Given a model `name`, return a path to its saved learning curves data\"\"\"\n",
    "    return MODEL_SAVE_FOLDER / Path(name + '.csv') \n",
    "\n",
    "def save_state(name : str, trainer : \"pl.Trainer\", metrics : \"MetricsCallback\"):\n",
    "    \"\"\"Save the model as a checkpoint, and also its learning curves data, under the specified `name`.\"\"\"\n",
    "    trainer.save_checkpoint(checkpoint_path(name))\n",
    "    \n",
    "    df = pd.DataFrame(metrics_callback.metrics)\n",
    "    df.to_csv(learn_curve_path(name), index=False)\n",
    "\n",
    "def load_state(model_class : \"pl.Module\", name : str) -> Tuple[\"pl.Module\", \"pd.DataFrame\"]:\n",
    "    \"\"\"Load the state named `name` for the model with class `model_class`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    instance : pl.Module\n",
    "        Model instantiated from the checkpoint\n",
    "    metrics : pd.DataFrame\n",
    "        DataFrame with the learning curves from the loaded training process.\n",
    "    \"\"\"\n",
    "    \n",
    "    instance = model_class.load_from_checkpoint(checkpoint_path(name))\n",
    "    \n",
    "    metrics = pd.read_csv(learn_curve_path(name))\n",
    "    \n",
    "    return instance, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning:\n",
      "\n",
      "Checkpoint directory SavedModels/cnn_autoencoder_first exists and is not empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar = LitProgressBar()\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=4, verbose=True)\n",
    "reco_callback = ReconstructedImage(dataset = mnist.val_dataset, every_n_epochs=1, directory=\"features/autoencoder/enc2\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='SavedModels/cnn_autoencoder_first', filename='cnn_autoencoder-{epoch:02d}-{val_loss:.2f}', save_top_k=5, mode='min')\n",
    "\n",
    "#Define training\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=50, callbacks=[checkpoint_callback, metrics_callback, reco_callback, bar, early_stopping_callback])\n",
    "\n",
    "#Define model\n",
    "cnn_autoencoder = ConvAutoEncoder(encoded_space_dim=2, learning_rate=1e-3, dropout=.4) #Start with a 2D encoding space, so that it can be easily plotted\n",
    "\n",
    "#Train\n",
    "#trainer.fit(cnn_autoencoder, mnist) \n",
    "# [UNCOMMENT] the previous line to repeat the training. Otherwise, the next cell will load a previous checkpoint.\n",
    "\n",
    "# Note: samples of reconstructed images from the validation dataset\n",
    "# are plotted in the folder \"features/autoencoder/enc2\" by the `reco_callback`.\n",
    "\n",
    "#Save model & learning curve\n",
    "#save_state(\"cnn_autoencoder_first\", trainer, metrics_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load saved model\n",
    "cnn_autoencoder, metrics = load_state(ConvAutoEncoder, \"cnn_autoencoder_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28976f0bf7fc4681aa357f030b41113a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot train/val reconstruction error at each epoch---#\n",
    "from plotting import plot_reconstruction_error\n",
    "\n",
    "plot_reconstruction_error(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d37cab4bb37414ba3c3826c5e5a2b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Explore the 2D representation space---#\n",
    "from plotting import plot_2D_representation_space\n",
    "\n",
    "n_samples = 500\n",
    "images, labels = mnist.val_dataset[:n_samples]\n",
    "encoded = cnn_autoencoder.encoder_cnn(images).data.numpy()\n",
    "\n",
    "x, y = encoded[:, 0], encoded[:, 1]\n",
    "images = images.squeeze(1).numpy()\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "    plot_2D_representation_space(x, y, images, labels, fig=fig)\n",
    "    \n",
    "    fig.savefig(\"Plots/2d_encoded_space.pdf\", transparent=True, bbox_inches='tight')\n",
    "    \n",
    "# NOTE: Move the mouse over the dots to see which digit is encoded there! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() #Close the previous plot when finished inspecting, so that no RAM is wasted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9650963377201613"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_batches = []\n",
    "label_batches = []\n",
    "\n",
    "#---Compute the latent representations of samples from the validation dataset---#\n",
    "cnn_autoencoder.eval()\n",
    "for (batch_x, y) in mnist.val_dataloader():\n",
    "    encoded_batches.append(cnn_autoencoder(batch_x).data.numpy())\n",
    "    label_batches.append(y.data.numpy())\n",
    "\n",
    "encoded_batches = np.vstack(encoded_batches)\n",
    "label_batches = np.hstack(label_batches)    \n",
    "\n",
    "from sklearn.metrics import davies_bouldin_score #https://www.wikiwand.com/en/Davies%E2%80%93Bouldin_index\n",
    "\n",
    "davies_bouldin_score(encoded_batches, label_batches)\n",
    "#The \"similarity\" of two clusters is defined as the ratio between their \"homogeneity\" (average distance of points that belong to a cluster to the centroid of that cluster) and their \"separation\" (distance between the two centroids). \n",
    "#The Davies Bouldin score measures the average similarity of each cluster with its most similar cluster. If clusters are well separated, the score is lower, down to a minimum value of 0 for \"perfect separation\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer: 17.934, Inner: 8.340\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "#---Compare the average distance between representations of the same digit, and representations of different digits---#\n",
    "outer_distances = []\n",
    "inner_distances = []\n",
    "for digit in range(10):\n",
    "    outer_distances.append(cdist(encoded_batches[label_batches == digit], encoded_batches[label_batches != digit]).mean())\n",
    "    inner_distances.append(pdist(encoded_batches[label_batches == digit]).mean())\n",
    "    \n",
    "print(f\"Outer: {np.mean(outer_distances):.3f}, Inner: {np.mean(inner_distances):.3f}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee259690727346b3aad63ed78186db84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Generate a sample based on position\n",
    "x, y = [0, -5]\n",
    "custom_encoded_value = torch.tensor([x, y]).float().unsqueeze(0)\n",
    "\n",
    "cnn_autoencoder.eval()\n",
    "with torch.no_grad():\n",
    "    generated_img = cnn_autoencoder.decoder_cnn(custom_encoded_value)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(generated_img.squeeze().cpu().numpy(), cmap='gist_gray')\n",
    "\n",
    "plt.title(f\"Image from ({x:.2f}, {y:.2f})\")\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92838cb742814334be7b77f65cef52bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Show a random reconstructed image from the validation set\n",
    "reco_callback = ReconstructedImage(dataset=mnist.val_dataset, every_n_epochs=1, show=True, directory=\"features/autoencoder/enc2/samples\")\n",
    "reco_callback.on_train_epoch_end(trainer, cnn_autoencoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Hyperparameter optimization with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Hyperparameter Optimization with Optuna---#\n",
    "\n",
    "def objective(trial: optuna.trial.Trial) -> float:\n",
    "    optimizer = trial.suggest_categorical(\"optimizer\", [\"SGD\", \"Adam\"])\n",
    "\n",
    "    encoded_space_dim = trial.suggest_int(\"encoded_space_dim\", 2, 50)\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-2, log=True)\n",
    "\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 1.0)\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [64, 128, 256])\n",
    "\n",
    "    #Convert to a dict \n",
    "    hyper_parameters = {\n",
    "        'optimizer' : optimizer,\n",
    "        'encoded_space_dim' : encoded_space_dim,\n",
    "        'learning_rate' : learning_rate,\n",
    "        'dropout_rate' : dropout_rate,\n",
    "        'batch_size' : batch_size\n",
    "    }\n",
    "\n",
    "    mnist = MNISTDataModule(batch_size=batch_size)\n",
    "    model = ConvAutoEncoder(hyper_parameters = hyper_parameters)\n",
    "\n",
    "    bar = LitProgressBar()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger = True,\n",
    "        limit_val_batches=1., #percentage of validation batches to be used\n",
    "        checkpoint_callback=False, #Do not save models during hyperparams opt.\n",
    "        max_epochs=10,\n",
    "        gpus = 1 if torch.cuda.is_available() else None,\n",
    "        callbacks = [PyTorchLightningPruningCallback(trial, monitor=\"val_loss\"), bar]\n",
    "    )\n",
    "\n",
    "    trainer.logger.log_hyperparams(hyper_parameters)\n",
    "    trainer.fit(model, datamodule=mnist)\n",
    "\n",
    "    return trainer.callback_metrics[\"val_loss\"].item() #Minimize \"val_loss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-21 00:49:33,935]\u001b[0m Using an existing study with name 'cnn_autoencoder' instead of creating a new one.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#---Execute Hyper Parameter Optimization---#\n",
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=30) #Prune (=terminate a trial early) if the trial's best intermediate result is worse than the median of intermediate results of previous trials at the same step. It is used to avoid wasting time evaluating hyperparameter choices that are \"really bad\".\n",
    "\n",
    "study = optuna.create_study(study_name=\"cnn_autoencoder\", storage=\"sqlite:///cnn_autoencoder.db\", direction=\"minimize\", pruner=pruner, load_if_exists=True)\n",
    "\n",
    "#[UNCOMMENT] this line if you want to execute the hyperparameters optimization (it can take a really long time)\n",
    "#Otherwise, the next cell will load the results\n",
    "#study.optimize(objective, n_trials=10, timeout=None) #timeout = stop after this many seconds (set to None to proceed without time limitation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-06-21 22:53:41,118]\u001b[0m Using an existing study with name 'cnn_autoencoder' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of finished trials: 100\n",
      "Best trial:\n",
      "  Value (val_loss): 0.010333129204809666\n",
      "  Params: \n",
      "    batch_size: 128\n",
      "    dropout_rate: 0.0011920463762665756\n",
      "    encoded_space_dim: 33\n",
      "    learning_rate: 0.005410311458704212\n",
      "    optimizer: Adam\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name=\"cnn_autoencoder\", storage=\"sqlite:///cnn_autoencoder.db\", direction=\"minimize\", load_if_exists=True)\n",
    "\n",
    "print(\"Number of finished trials: {}\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"  Value (val_loss): {}\".format(best_trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optuna.visualization import plot_optimization_history\n",
    "from optuna.visualization import plot_parallel_coordinate\n",
    "\n",
    "# #plot_optimization_history(study)\n",
    "fig = plot_parallel_coordinate(study)\n",
    "fig.write_image(\"Plots/hyperparameters.pdf\")\n",
    "#this may require plotly-orca to be installed with conda:\n",
    "#conda install -c plotly plotly-orca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Train with best hyperparameters---#\n",
    "best_hyperparameters = best_trial.params\n",
    "\n",
    "bar = LitProgressBar()\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3, verbose=True)\n",
    "reco_callback = ReconstructedImage(dataset = mnist.val_dataset, every_n_epochs=1, directory=\"features/autoencoder/best\")\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='SavedModels/experiments', filename='cnn_autoencoder-{epoch:02d}-{val_loss:.2f}', save_top_k=5, mode='min')\n",
    "\n",
    "#Define training\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, callbacks=[checkpoint_callback, metrics_callback, reco_callback, bar, early_stopping_callback])\n",
    "\n",
    "#Define data (with the correct batch_size)\n",
    "mnist = MNISTDataModule(batch_size=best_hyperparameters['batch_size'])\n",
    "\n",
    "#Define model\n",
    "cnn_autoencoder = ConvAutoEncoder(hyper_parameters=best_hyperparameters) #Start with a 2D encoding space, so that it can be easily plotted\n",
    "\n",
    "#Train\n",
    "#trainer.fit(cnn_autoencoder, mnist)\n",
    "# [UNCOMMENT] the previous line to repeat the training. Otherwise, the next cell will load a previous checkpoint.\n",
    "\n",
    "# Note: samples of reconstructed images from the validation dataset\n",
    "# are plotted in the folder \"features/autoencoder/best\" by the `reco_callback`.\n",
    "\n",
    "#save_state(\"cnn_autoencoder_best\", trainer, metrics_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Load results from previous training--#\n",
    "cnn_autoencoder, metrics = load_state(ConvAutoEncoder, \"cnn_autoencoder_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350ee532868f41e993fe662eef560b8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot train/val reconstruction error at each epoch---#\n",
    "from plotting import plot_reconstruction_error\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig, ax = plt.subplots(figsize=latex_figsize(wf=1, columnwidth=COLUMNWIDTH))\n",
    "\n",
    "    plot_reconstruction_error(metrics, ax=ax)\n",
    "\n",
    "    fig.savefig(\"Plots/best_hyperparams_learning_curve.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning:\n",
      "\n",
      "The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 VALIDATE RESULTS\n",
      "{'val_loss': 0.008700749836862087}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.008700749836862087}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model=cnn_autoencoder, datamodule=mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning:\n",
      "\n",
      "The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420cb05ae4c744eba872844c82d3ea8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.00853609200567007, 'val_loss': 0.008700749836862087}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.008700749836862087, 'test_loss': 0.00853609200567007}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Estimate val/test loss\n",
    "trainer.test(model=cnn_autoencoder, datamodule=mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d672bf89b5f4ee4a1e072a5cdd09f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot reconstructed samples---#\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "n_samples = 9\n",
    "MAX_PER_ROW = 3\n",
    "\n",
    "n_rows = math.ceil(n_samples / MAX_PER_ROW)\n",
    "n_cols = min(n_samples, MAX_PER_ROW)\n",
    "\n",
    "cnn_autoencoder.eval()\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "\n",
    "    ext_grid = gridspec.GridSpec(n_rows, n_cols, figure=fig)\n",
    "\n",
    "    for place in ext_grid:\n",
    "        in_grid = place.subgridspec(1, 2)\n",
    "\n",
    "        ax_orig = fig.add_subplot(in_grid[0])\n",
    "        ax_reco = fig.add_subplot(in_grid[1])\n",
    "\n",
    "        ax_orig.axis('off')\n",
    "        ax_reco.axis('off')\n",
    "\n",
    "        x, _ = random.choice(mnist.val_dataset)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded = cnn_autoencoder.encoder_cnn(x.unsqueeze(0))\n",
    "            reco = cnn_autoencoder.decoder_cnn(encoded)\n",
    "            \n",
    "        ax_orig.set_title('Orig.', fontsize=10)\n",
    "        ax_reco.set_title('Reco.', fontsize=10)\n",
    "\n",
    "        ax_orig.imshow(x.numpy().reshape(28, 28), cmap='gist_gray')\n",
    "        ax_reco.imshow(reco.numpy().reshape(28, 28), cmap='gist_gray')\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    plt.suptitle(\"(Original, Reconstructed) samples\")\n",
    "\n",
    "    plt.tight_layout(w_pad=1.5, h_pad=3)\n",
    "    fig.savefig(\"Plots/orig_reconstructed.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Latent Space Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "encoded_batches = []\n",
    "label_batches = []\n",
    "\n",
    "#---Compute the latent representations of samples from the validation dataset---#\n",
    "cnn_autoencoder.eval()\n",
    "for (batch_x, y) in mnist.val_dataloader():\n",
    "    encoded_batches.append(cnn_autoencoder.encoder_cnn(batch_x).data.numpy())\n",
    "    label_batches.append(y.data.numpy())\n",
    "\n",
    "encoded_batches = np.vstack(encoded_batches)\n",
    "label_batches = np.hstack(label_batches)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: [0.20324522 0.11365472]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44fb689ceaa24e18851226a4a0efe356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "#---PCA projection---#\n",
    "pca = PCA(n_components=2)\n",
    "projected_features = pca.fit_transform(encoded_batches)\n",
    "\n",
    "print(f\"Explained variance: {pca.explained_variance_ratio_}\")\n",
    "\n",
    "colors = get_cmap('tab10').colors\n",
    "point_colors = np.array(colors)[label_batches]\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig, ax = plt.subplots(figsize=latex_figsize(1., columnwidth=COLUMNWIDTH))\n",
    "    ax.scatter(projected_features[:, 0], projected_features[:, 1], c=point_colors, s=1)\n",
    "    ax.set_title(\"PCA (Validation Dataset)\") #Analysis on the autoencoder with best hyperparameters\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'{i}',\n",
    "                            markerfacecolor=colors[i]) for i in range(10)]\n",
    "    ax.legend(handles=legend_elements, ncol=5, columnspacing=.5, handletextpad=-.1) #Save this image\n",
    "\n",
    "    ax.patch.set_facecolor(\"white\")\n",
    "    \n",
    "    ax.set_xlabel(\"var0\")\n",
    "    ax.set_ylabel(\"var1\")\n",
    "    \n",
    "    fig.savefig(\"Plots/best_hyperparams_PCA.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.764029853970084"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score #https://www.wikiwand.com/en/Davies%E2%80%93Bouldin_index\n",
    "\n",
    "davies_bouldin_score(encoded_batches, label_batches)\n",
    "#The \"similarity\" of two clusters is defined as the ratio between their \"homogeneity\" (average distance of points that belong to a cluster to the centroid of that cluster) and their \"separation\" (distance between the two centroids). \n",
    "#The Davies Bouldin score measures the average similarity of each cluster with its most similar cluster. If clusters are well separated, the score is lower, down to a minimum value of 0 for \"perfect separation\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.243625314312094"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "davies_bouldin_score(projected_features, label_batches) #Note that in the projected space points are not separated very well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9393163735679441"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---t-SNE---#\n",
    "tsne_projected = TSNE(n_components=2).fit_transform(encoded_batches) #Warning: this computation takes a bit of time!\n",
    "\n",
    "davies_bouldin_score(tsne_projected, label_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99baa40b7c674ab39f8807ff857f4f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot t-SNE projection---#\n",
    "from matplotlib.cm import get_cmap\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "colors = get_cmap('tab10').colors\n",
    "point_colors = np.array(colors)[label_batches]\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig, ax = plt.subplots(figsize=latex_figsize(1., columnwidth=COLUMNWIDTH))\n",
    "    ax.scatter(tsne_projected[:, 0], projected_features[:, 1], c=point_colors, s=1)\n",
    "    ax.set_title(\"t-SNE (Validation Dataset)\") \n",
    "    \n",
    "    ax.set_xlabel(\"var0\")\n",
    "    ax.set_ylabel(\"var1\")\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], marker='o', color='w', label=f'{i}',\n",
    "                            markerfacecolor=colors[i]) for i in range(10)]\n",
    "    ax.legend(handles=legend_elements, ncol=5, columnspacing=.5, handletextpad=-.1)\n",
    "    ax.patch.set_facecolor(\"white\")\n",
    "\n",
    "    fig.savefig(\"Plots/best_hyperparams_tSNE.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Denoising Convolutional AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MNISTDataModule' object has no attribute 'train_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-e270674d6754>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAddGaussianNoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAddSaltPepperNoise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAddSpeckleNoise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mx_clean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mx_gaussian\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mAddGaussianNoise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MNISTDataModule' object has no attribute 'train_dataset'"
     ]
    }
   ],
   "source": [
    "#---Noise types---#\n",
    "from data import AddGaussianNoise, AddSaltPepperNoise, AddSpeckleNoise\n",
    "\n",
    "x_clean, y = mnist.train_dataset[10]\n",
    "\n",
    "x_gaussian   = AddGaussianNoise(0.3, .05)(x_clean)\n",
    "x_saltpepper = AddSaltPepperNoise(salt_vs_pepper=.5, amount=.05)(x_clean)\n",
    "x_speckle    = AddSpeckleNoise(0., .5)(x_clean)\n",
    "\n",
    "all_noises = torchvision.transforms.Compose([\n",
    "    AddSpeckleNoise(),\n",
    "    AddSaltPepperNoise(salt_vs_pepper=.5, amount=.1),\n",
    "    AddGaussianNoise(.5, .3)\n",
    "])\n",
    "\n",
    "x_all_noise = all_noises(x_clean)\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig, axes = plt.subplots(nrows=1, ncols=5, figsize=latex_figsize(wf=1., hf=.28, columnwidth=COLUMNWIDTH))\n",
    "    plt.subplots_adjust(wspace=.05)\n",
    "\n",
    "    tensors = [x_clean, x_gaussian, x_saltpepper, x_speckle, x_all_noise]\n",
    "    labels  = [\"Clean\", \"+ Gauss\", \"+ S\\&P\", \"+ Speckle\", \"+ All\"]\n",
    "\n",
    "    for i, ax in enumerate(axes):\n",
    "        ax.imshow(tensors[i].squeeze(0).data.numpy(), cmap='gist_gray')\n",
    "        ax.axis('off')\n",
    "        ax.set_title(f\"{labels[i]}\")\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_yticklabels([])\n",
    "        \n",
    "        ax.patch.set_facecolor(\"white\")\n",
    "    \n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    plt.title(\"Noise Types\")\n",
    "    fig.savefig(\"Plots/noise_types.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Create noisy dataset---#\n",
    "from data import MNISTNoisy\n",
    "\n",
    "mnist_noisy = MNISTNoisy(noise_transform=all_noises, batch_size=best_hyperparameters['batch_size'])\n",
    "mnist_noisy.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenoisingAutoEncoder(pl.LightningModule):\n",
    "    \"\"\"Denoising Convolutional AutoEncoder for MNIST\"\"\"\n",
    "    \n",
    "    #x_out = floor([x_in + 2 * x_pad - (x_ker - 1) - 1] / x_stride + 1)\n",
    "\n",
    "    def __init__(self, hyper_parameters : dict = None, *args, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        if hyper_parameters is None:\n",
    "            self.hyper_parameters = { #Default values\n",
    "                'optimizer' : 'Adam',\n",
    "                'encoded_space_dim' : 10,\n",
    "                'learning_rate' : 1e-3,\n",
    "                'dropout_rate' : .1\n",
    "            }\n",
    "            self.hyper_parameters.update(**kwargs)\n",
    "        else:\n",
    "            self.hyper_parameters = hyper_parameters\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.autoencoder = ConvAutoEncoder(hyper_parameters=hyper_parameters) #Copy the same architecture used for the initial AutoEncoder\n",
    "\n",
    "        self.dropout_rate = self.hyper_parameters['dropout_rate']\n",
    "        self.encoded_space_dim = self.hyper_parameters['encoded_space_dim']\n",
    "\n",
    "        self.encoder_cnn = self.autoencoder.encoder_cnn\n",
    "        self.decoder_cnn = self.autoencoder.decoder_cnn\n",
    "    \n",
    "    def forward(self, x : \"torch.tensor\"):\n",
    "        embedding = self.encoder_cnn(x)\n",
    "\n",
    "        return embedding\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, log_name='train_loss', prog_bar=False):\n",
    "        x_clean, x_noisy, _ = batch #ignore labels\n",
    "\n",
    "        internal_repr = self.encoder_cnn(x_noisy)\n",
    "        x_hat = self.decoder_cnn(internal_repr)\n",
    "\n",
    "        loss = F.mse_loss(x_hat, x_clean)\n",
    "        self.log(log_name, loss, prog_bar=prog_bar)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = getattr(optim, self.hyper_parameters['optimizer'])(self.parameters(), lr=self.hyper_parameters['learning_rate'])\n",
    "        return optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, log_name='val_loss', prog_bar=True):\n",
    "        return self.training_step(batch, batch_idx, log_name=log_name, prog_bar=prog_bar)\n",
    "\n",
    "    def test_step(self, batch, batch_idx, log_name='test_loss'):\n",
    "        \n",
    "        return self.validation_step(batch, batch_idx, log_name=log_name, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning:\n",
      "\n",
      "Checkpoint directory SavedModels/denoising exists and is not empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics_callback = MetricsCallback()\n",
    "reco_callback = ReconstructedImage(dataset = mnist_noisy.val_dataset, every_n_epochs=1, directory = \"features/denoising/\", pick_noisy=True)\n",
    "bar = LitProgressBar()\n",
    "checkpoint_callback = ModelCheckpoint(monitor='val_loss', dirpath='SavedModels/denoising', filename='denoising_autoencoder-{epoch:02d}-{val_loss:.2f}', save_top_k=5, mode='min')\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=4, verbose=True)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=50, callbacks=[metrics_callback, reco_callback, bar, checkpoint_callback, early_stopping_callback]) #reco_callback (show noisy image and reconstructed)\n",
    "\n",
    "denoise_autoencoder = DenoisingAutoEncoder(hyper_parameters=best_hyperparameters) #encoded_space_dim\n",
    "\n",
    "#trainer.fit(denoise_autoencoder, mnist_noisy) \n",
    "# [UNCOMMENT] the previous line to repeat the training. Otherwise, the next cell will load a previous checkpoint.\n",
    "\n",
    "# Note: samples of reconstructed images from the validation dataset\n",
    "# are plotted in the folder \"features/denoising\" by the `reco_callback`.\n",
    "\n",
    "#Save model\n",
    "#save_state(\"cnn_denoiser\", trainer, metrics_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_denoiser, metrics = load_state(DenoisingAutoEncoder, \"cnn_denoiser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2107de4004ee4be2bb1b3b322c038c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 0.02999124675989151}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 0.02999124675989151}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(cnn_denoiser, mnist_noisy.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f39c3d983b84d04844c4e5e37b59a01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot denoised samples---#\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "n_samples = 9\n",
    "MAX_PER_ROW = 3\n",
    "\n",
    "n_rows = math.ceil(n_samples / MAX_PER_ROW)\n",
    "n_cols = min(n_samples, MAX_PER_ROW)\n",
    "\n",
    "cnn_denoiser.eval()\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "\n",
    "    ext_grid = gridspec.GridSpec(n_rows, n_cols, figure=fig)\n",
    "\n",
    "    for place in ext_grid:\n",
    "        in_grid = place.subgridspec(1, 2)\n",
    "\n",
    "        ax_noisy = fig.add_subplot(in_grid[0])\n",
    "        ax_denoisy = fig.add_subplot(in_grid[1])\n",
    "\n",
    "        ax_noisy.axis('off')\n",
    "        ax_denoisy.axis('off')\n",
    "\n",
    "        x, x_noisy, y = random.choice(mnist_noisy.val_dataset)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded = cnn_denoiser.encoder_cnn(x_noisy.unsqueeze(0))\n",
    "            reco = cnn_denoiser.decoder_cnn(encoded)\n",
    "\n",
    "        ax_noisy.set_title(f'\"{y}\"')\n",
    "\n",
    "        ax_noisy.imshow(x_noisy.numpy().reshape(28, 28), cmap='gist_gray')\n",
    "        ax_denoisy.imshow(reco.numpy().reshape(28, 28), cmap='gist_gray')\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    plt.suptitle(\"(Noisy, Denoised) samples\")\n",
    "\n",
    "    plt.tight_layout(w_pad=1.5, h_pad=3)\n",
    "    fig.savefig(\"Plots/noisy_denoised.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.052746754689131"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score #https://www.wikiwand.com/en/Davies%E2%80%93Bouldin_index\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "encoded_batches = []\n",
    "label_batches = []\n",
    "\n",
    "#---Compute the latent representations of samples from the validation dataset---#\n",
    "cnn_denoiser.eval()\n",
    "for (batch_x, y) in mnist.val_dataloader():\n",
    "    encoded_batches.append(cnn_denoiser.encoder_cnn(batch_x).data.numpy())\n",
    "    label_batches.append(y.data.numpy())\n",
    "\n",
    "encoded_batches = np.vstack(encoded_batches)\n",
    "label_batches = np.hstack(label_batches)    \n",
    "\n",
    "davies_bouldin_score(encoded_batches, label_batches)\n",
    "#The \"similarity\" of two clusters is defined as the ratio between their \"homogeneity\" (average distance of points that belong to a cluster to the centroid of that cluster) and their \"separation\" (distance between the two centroids). \n",
    "#The Davies Bouldin score measures the average similarity of each cluster with its most similar cluster. If clusters are well separated, the score is lower, down to a minimum value of 0 for \"perfect separation\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Supervised FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Fine-tune the (convolutional) autoencoder using a supervised classification task \n",
    "#You can compare the classification accuracy and learning speed with results achieved in homework 1.\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "class SupervisedFineTuner(pl.LightningModule):\n",
    "    \"\"\"Denoising Convolutional AutoEncoder for MNIST\"\"\"\n",
    "    \n",
    "    #x_out = floor([x_in + 2 * x_pad - (x_ker - 1) - 1] / x_stride + 1)\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.autoencoder = ConvAutoEncoder.load_from_checkpoint(checkpoint_path(\"cnn_autoencoder_best\"))\n",
    "        self.autoencoder.freeze() #Do not train this part\n",
    "        \n",
    "        self.encoder = self.autoencoder.encoder_cnn\n",
    "        \n",
    "        self.encoded_space_dim = self.autoencoder.encoded_space_dim\n",
    "\n",
    "        self.supervised_segment = nn.Sequential(\n",
    "            nn.Linear(self.encoded_space_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            #nn.Dropout(.1),\n",
    "            nn.Linear(128, 10),\n",
    "            #nn.ReLU(),\n",
    "            #nn.Dropout(.1),\n",
    "            #nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "        #Accuracy metric\n",
    "        self.metric = torchmetrics.Accuracy()\n",
    "    \n",
    "    def forward(self, x : \"torch.tensor\"):\n",
    "        x = self.encoder(x)\n",
    "        x = self.supervised_segment(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, log_name='train'):\n",
    "        x, y = batch #ignore labels\n",
    "\n",
    "        out = self.forward(x)\n",
    "        probas = F.softmax(out)\n",
    "\n",
    "        loss = F.cross_entropy(out, y)\n",
    "        acc  = self.metric(probas, y)\n",
    "\n",
    "        self.log(log_name + '_loss', loss)\n",
    "        self.log(log_name + '_acc', acc, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=5e-3)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, log_name='val'):\n",
    "\n",
    "        return self.training_step(batch, batch_idx, log_name=log_name)\n",
    "\n",
    "    def test_step(self, batch, batch_idx, log_name='test'):\n",
    "        \n",
    "        return self.validation_step(batch, batch_idx, log_name=log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "             Kernel Shape    Output Shape  Params Mult-Adds\n",
      "Layer                                                      \n",
      "0_metric     [1, 8, 3, 3]  [1, 8, 14, 14]       -         -\n",
      "1_metric     [1, 8, 3, 3]  [1, 8, 14, 14]       -   14.112k\n",
      "2_metric                -  [1, 8, 14, 14]       -         -\n",
      "3_metric                -  [1, 8, 14, 14]       -         -\n",
      "4_metric                -  [1, 8, 14, 14]       -         -\n",
      "5_metric                -  [1, 8, 14, 14]       -         -\n",
      "6_metric    [8, 16, 3, 3]   [1, 16, 7, 7]       -         -\n",
      "7_metric    [8, 16, 3, 3]   [1, 16, 7, 7]       -   56.448k\n",
      "8_metric                -   [1, 16, 7, 7]       -         -\n",
      "9_metric                -   [1, 16, 7, 7]       -         -\n",
      "10_metric               -   [1, 16, 7, 7]       -         -\n",
      "11_metric               -   [1, 16, 7, 7]       -         -\n",
      "12_metric  [16, 32, 3, 3]   [1, 32, 3, 3]       -         -\n",
      "13_metric  [16, 32, 3, 3]   [1, 32, 3, 3]       -   41.472k\n",
      "14_metric               -   [1, 32, 3, 3]       -         -\n",
      "15_metric               -   [1, 32, 3, 3]       -         -\n",
      "16_metric               -   [1, 32, 3, 3]       -         -\n",
      "17_metric               -   [1, 32, 3, 3]       -         -\n",
      "18_metric               -        [1, 288]       -         -\n",
      "19_metric               -        [1, 288]       -         -\n",
      "20_metric       [288, 64]         [1, 64]       -         -\n",
      "21_metric       [288, 64]         [1, 64]       -   18.432k\n",
      "22_metric               -         [1, 64]       -         -\n",
      "23_metric               -         [1, 64]       -         -\n",
      "24_metric               -         [1, 64]       -         -\n",
      "25_metric               -         [1, 64]       -         -\n",
      "26_metric        [64, 33]         [1, 33]       -         -\n",
      "27_metric        [64, 33]         [1, 33]       -    2.112k\n",
      "28_metric       [33, 128]        [1, 128]  4.352k    4.224k\n",
      "29_metric               -        [1, 128]       -         -\n",
      "30_metric       [128, 10]         [1, 10]   1.29k     1.28k\n",
      "------------------------------------------------------------\n",
      "                       Totals\n",
      "Total params            58.7k\n",
      "Trainable params       5.642k\n",
      "Non-trainable params  53.058k\n",
      "Mult-Adds             138.08k\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "\n",
    "metrics_callback = MetricsCallback()\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=True)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, callbacks=[metrics_callback, early_stopping_callback]) #reco_callback (show noisy image and reconstructed)\n",
    "\n",
    "supervised_fine_tuned = SupervisedFineTuner()\n",
    "\n",
    "supervised_summary = summary(supervised_fine_tuned, torch.zeros((1, 1, 28, 28)))\n",
    "#Note that only (relatively) few parameters are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.fit(supervised_fine_tuned, mnist)\n",
    "# [UNCOMMENT] the previous line to repeat the training. Otherwise, the next cell will load a previous checkpoint.\n",
    "\n",
    "#Save model & learning curve\n",
    "#save_state(\"cnn_autoencoder_supervised\", trainer, metrics_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_fine_tuned, metrics = load_state(SupervisedFineTuner, \"cnn_autoencoder_supervised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4973f3ec281246ea86be365f5c8ebc26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-8-286a28072247>:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  probas = F.softmax(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9887708425521851, 'test_loss': 0.03757133707404137}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.9887708425521851, 'test_loss': 0.03757133707404137}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=supervised_fine_tuned, test_dataloaders=mnist.train_dataloader()) #Train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "557d336df7fc4e809348fbe57229e360",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-286a28072247>:44: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.9889166951179504, 'test_loss': 0.035658761858940125}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.9889166951179504, 'test_loss': 0.035658761858940125}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=supervised_fine_tuned, test_dataloaders=mnist.val_dataloader()) #Validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eccc94800474b2aa796c8dc7def821e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-40-286a28072247>:44: UserWarning:\n",
      "\n",
      "Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_acc': 0.979200005531311, 'test_loss': 0.08013071119785309}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc': 0.979200005531311, 'test_loss': 0.08013071119785309}]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model=supervised_fine_tuned, test_dataloaders=mnist.test_dataloader()) #Test error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d5b907c2eb40ec9e128e28eddd37d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import ticker\n",
    "\n",
    "#---Plot learning curves---#\n",
    "with mpl2latex(True):\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "    \n",
    "    ax.plot(metrics[\"train_loss\"], label=\"train\", c='k')\n",
    "    ax.plot(metrics[\"val_loss\"], label=\"val\", c='r')\n",
    "    \n",
    "    ax.set_yscale('log')\n",
    "    \n",
    "    ax.set_title(\"Learning curves - Autoencoder + Supervised\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.legend()\n",
    "\n",
    "    formatter = ticker.ScalarFormatter(useMathText=True) #Put the multiplier (e.g. *1e-2) at the top left side, above the plot, making everything more \"compact\"\n",
    "    formatter.set_scientific(True) \n",
    "    formatter.set_powerlimits((-1,1)) \n",
    "\n",
    "    ax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "    ax.patch.set_facecolor('white')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    fig.savefig(\"Plots/learning_curves_supervised.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(pl.LightningModule):\n",
    "    def __init__(self, encoded_space_dim : int = 10):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encoded_space_dim = encoded_space_dim\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.encoder_cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3, stride=2, padding=1), #out = (8, 14, 14)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, kernel_size=3, stride=2, padding=1), #out = (16, 7, 7)\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0), #out = (32, 3, 3)\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(start_dim=1)\n",
    "        )\n",
    "\n",
    "        self.decoder_cnn = nn.Sequential( #Specular\n",
    "            nn.Linear(self.encoded_space_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 3 * 3 * 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Unflatten(dim=1, unflattened_size=(32, 3, 3)),\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, output_padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 1, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.Sigmoid() #-> [0,1] output\n",
    "        )\n",
    "\n",
    "        self.avg = nn.Sequential( #Predicts the means of a multi-variate gaussian\n",
    "            nn.Linear(3 * 3 * 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.encoded_space_dim)\n",
    "        )\n",
    "\n",
    "        self.log_var = nn.Sequential( #Predicts the (log) variances of a multi-variate gaussian (diagonal of the covariance matrix)\n",
    "            nn.Linear(3 * 3 * 32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, self.encoded_space_dim)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _variational_loss(x_hat, x_true, mu, log_var):\n",
    "        \"\"\"\n",
    "        Loss for a variational autoencoder, from https://arxiv.org/pdf/1312.6114.pdf\n",
    "        \"\"\"\n",
    "\n",
    "        reco_loss = F.mse_loss(x_hat, x_true, reduction='sum') #Both losses should be aggregated in the same way over a batch\n",
    "        #otherwise, if one is averaged and the other summed, they will be of different magnitude, and one will be minimized preferentially\n",
    "        #making convergence difficult\n",
    "     \n",
    "        kl_div    = -0.5 * torch.sum(1. + log_var - mu**2 - torch.exp(log_var))\n",
    "\n",
    "        return reco_loss + kl_div\n",
    "\n",
    "    @staticmethod\n",
    "    def _sample(mu, log_var):\n",
    "        \"\"\"\n",
    "        Generate samples from a multi-variate Normal distribution with diagonal covariance matrix, given a vector of means `mu` and \n",
    "        the vector of log(variances) `log_var`.\n",
    "        \"\"\"\n",
    "\n",
    "        sigma = torch.exp(0.5 * log_var) #log_var is log(variance) = log(sigma**2) = 2 * log(sigma)\n",
    "\n",
    "        return mu + torch.randn_like(mu) * sigma\n",
    "        #Var(aX) = a**2 Var(X), so we need to multiply pred_sqrt_var = sigma (square root of the variance) by the standard normal distribution\n",
    "\n",
    "    def forward(self, x):\n",
    "        internal_repr = self.encoder_cnn(x)\n",
    "\n",
    "        pred_means   = self.avg(internal_repr)\n",
    "        pred_log_var = self.log_var(internal_repr) \n",
    "\n",
    "        #Produce a sample\n",
    "        sample = self._sample(mu=pred_means, log_var=pred_log_var)\n",
    "        \n",
    "        return sample, pred_means, pred_log_var\n",
    "\n",
    "    def training_step(self, batch, batch_idx, log_name='train', prog_bar=False):\n",
    "        x, _ = batch\n",
    "        sample, pred_means, pred_log_var = self.forward(x)\n",
    "\n",
    "        reconstructed = self.decoder_cnn(sample)\n",
    "        loss = self._variational_loss(x_hat=reconstructed, x_true=x, mu=pred_means, log_var=pred_log_var)\n",
    "\n",
    "        self.log(log_name + '_loss', loss, prog_bar=prog_bar)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def validation_step(self, batch, batch_idx, log_name='val'):\n",
    "\n",
    "        return self.training_step(batch, batch_idx, log_name=log_name, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx, log_name='test'):\n",
    "        \n",
    "        return self.training_step(batch, batch_idx, log_name=log_name, prog_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning:\n",
      "\n",
      "The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n",
      "C:\\Users\\franc\\Anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning:\n",
      "\n",
      "The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 16 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de74bc2920a444529edb3651a0a5e570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metrics_callback = MetricsCallback()\n",
    "bar = LitProgressBar()\n",
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=10, verbose=True)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_epochs=100, callbacks=[metrics_callback, bar, early_stopping_callback]) #reco_callback (show noisy image and reconstructed)\n",
    "\n",
    "variational_autoencoder = VariationalAutoEncoder(encoded_space_dim=33) #2D latent space, so that it can be easily visualized\n",
    "\n",
    "trainer.fit(variational_autoencoder, mnist) \n",
    "# [UNCOMMENT] the previous line to repeat the training. Otherwise, the next cell will load a previous checkpoint.\n",
    "\n",
    "save_state(\"vae_autoencoder\", trainer, metrics_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "variational_autoencoder, metrics = load_state(VariationalAutoEncoder, \"vae_autoencoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186ede510e88415d92b1f92cec8e1585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'test_loss': 1788.4122314453125}\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 1788.4122314453125}]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(variational_autoencoder, mnist.test_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e6437433f54416bf21006ec6da3000",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot reconstructed samples---#\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "n_samples = 9\n",
    "MAX_PER_ROW = 3\n",
    "\n",
    "n_rows = math.ceil(n_samples / MAX_PER_ROW)\n",
    "n_cols = min(n_samples, MAX_PER_ROW)\n",
    "\n",
    "variational_autoencoder.eval()\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "\n",
    "    ext_grid = gridspec.GridSpec(n_rows, n_cols, figure=fig)\n",
    "\n",
    "    for place in ext_grid:\n",
    "        in_grid = place.subgridspec(1, 2)\n",
    "\n",
    "        ax_orig = fig.add_subplot(in_grid[0])\n",
    "        ax_reco = fig.add_subplot(in_grid[1])\n",
    "\n",
    "        ax_orig.axis('off')\n",
    "        ax_reco.axis('off')\n",
    "\n",
    "        x, _ = random.choice(mnist.val_dataset)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            encoded, _, _ = variational_autoencoder(x.unsqueeze(0))\n",
    "            reco = variational_autoencoder.decoder_cnn(encoded)\n",
    "            \n",
    "        ax_orig.set_title('Orig.', fontsize=10)\n",
    "        ax_reco.set_title('Reco.', fontsize=10)\n",
    "\n",
    "        ax_orig.imshow(x.numpy().reshape(28, 28), cmap='gist_gray')\n",
    "        ax_reco.imshow(reco.numpy().reshape(28, 28), cmap='gist_gray')\n",
    "\n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    plt.suptitle(\"(Original, Reconstructed) samples\")\n",
    "\n",
    "    plt.tight_layout(w_pad=1.5, h_pad=3)\n",
    "    fig.savefig(\"Plots/orig_reconstructed_vae.pdf\", transparent=True, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97413056b7de40f6b9944e0a309a9a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "variational_autoencoder.eval()\n",
    "\n",
    "start_img, y1 = mnist.val_dataset[103]\n",
    "end_img, y2 = mnist.val_dataset[18]\n",
    "\n",
    "assert y1 != y2, \"Please change the indices for start_img and end_img\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    start_pos = variational_autoencoder.avg(variational_autoencoder.encoder_cnn(start_img.unsqueeze(0))).data.numpy().flatten()\n",
    "    end_pos = variational_autoencoder.avg(variational_autoencoder.encoder_cnn(end_img.unsqueeze(0))).data.numpy().flatten()\n",
    "    \n",
    "steps = 2 + 5\n",
    "line = np.linspace(0, 1, 7)\n",
    "points = [start_pos + val * (end_pos - start_pos) for val in line]\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig, axs = plt.subplots(1, steps, figsize=latex_figsize(wf=1., hf=.2, columnwidth=COLUMNWIDTH))\n",
    "    \n",
    "    for i, (ax, point) in enumerate(zip(axs.ravel(), points)):\n",
    "        with torch.no_grad():\n",
    "            img = variational_autoencoder.decoder_cnn(torch.tensor(point).unsqueeze(0))\n",
    "        \n",
    "        if i == 0:\n",
    "            ax.set_title(f\"Start: {y1}\")\n",
    "        if i == steps-1:\n",
    "            ax.set_title(f\"End: {y2}\")\n",
    "            \n",
    "        ax.imshow(img.squeeze().numpy(), cmap='gist_gray')\n",
    "        ax.axis('off')\n",
    "        \n",
    "    fig.add_subplot(111, frameon=False)\n",
    "    plt.tick_params(labelcolor='none', which='both', top=False, bottom=False, left=False, right=False)\n",
    "    plt.suptitle('\"Transition\" between two samples')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"Plots/vae_transition.pdf\", transparent=True, bbox_inches='tight')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "encoded_batches = []\n",
    "label_batches = []\n",
    "\n",
    "#---Compute the latent representations of samples from the validation dataset---#\n",
    "variational_autoencoder.eval()\n",
    "for (batch_x, y) in mnist.val_dataloader():\n",
    "    encoded_batches.append(variational_autoencoder.avg(variational_autoencoder.encoder_cnn(batch_x)).data.numpy())\n",
    "    label_batches.append(y.data.numpy())\n",
    "\n",
    "encoded_batches = np.vstack(encoded_batches)\n",
    "label_batches = np.hstack(label_batches)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.3851832225609"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import davies_bouldin_score #https://www.wikiwand.com/en/Davies%E2%80%93Bouldin_index\n",
    "\n",
    "davies_bouldin_score(encoded_batches, label_batches)\n",
    "#The \"similarity\" of two clusters is defined as the ratio between their \"homogeneity\" (average distance of points that belong to a cluster to the centroid of that cluster) and their \"separation\" (distance between the two centroids). \n",
    "#The Davies Bouldin score measures the average similarity of each cluster with its most similar cluster. If clusters are well separated, the score is lower, down to a minimum value of 0 for \"perfect separation\". \n",
    "\n",
    "#It is higher here, since the clusters are \"less dense\" (as it can be seen from the 2d plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer: 1.836, Inner: 1.051\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cdist, pdist\n",
    "\n",
    "#---Compare the average distance between representations of the same digit, and representations of different digits---#\n",
    "outer_distances = []\n",
    "inner_distances = []\n",
    "for digit in range(10):\n",
    "    outer_distances.append(cdist(encoded_batches[label_batches == digit], encoded_batches[label_batches != digit]).mean())\n",
    "    inner_distances.append(pdist(encoded_batches[label_batches == digit]).mean())\n",
    "    \n",
    "print(f\"Outer: {np.mean(outer_distances):.3f}, Inner: {np.mean(inner_distances):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f56ed4dbfd45acb70cc3c468e4e097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#---Plot 2D representation space---#\n",
    "variational_autoencoder, metrics = load_state(VariationalAutoEncoder, \"vae_autoencoder2d\") \n",
    "#load the vae trained with encoded_space_dim=2\n",
    "\n",
    "from plotting import plot_2D_representation_space\n",
    "\n",
    "n_samples = 500\n",
    "variational_autoencoder.eval()\n",
    "images, labels = mnist.val_dataset[:n_samples]\n",
    "encoded = variational_autoencoder.avg(variational_autoencoder.encoder_cnn(images)).data.numpy()\n",
    "\n",
    "x, y = encoded[:, 0], encoded[:, 1]\n",
    "images = images.squeeze(1).numpy()\n",
    "\n",
    "with mpl2latex(True):\n",
    "    fig = plt.figure(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "    plot_2D_representation_space(x, y, images, labels, fig=fig)\n",
    "    \n",
    "    fig.savefig(\"Plots/2d_encoded_space_vae.pdf\", transparent=True, bbox_inches='tight')\n",
    "\n",
    "# NOTE: Move your mouse on the dots to see which image is encoded at that position!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close() #Close plot to not waste RAM"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nndl_2020__homework_2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
