
@online{dueling_networks,
  title = {Dueling {{Network Architectures}} for {{Deep Reinforcement Learning}}},
  author = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and van Hasselt, Hado and Lanctot, Marc and de Freitas, Nando},
  date = {2016-04-05},
  url = {http://arxiv.org/abs/1511.06581},
  abstract = {In recent years there have been many successes of using deep representations in reinforcement learning. Still, many of these applications use conventional architectures, such as convolutional networks, LSTMs, or auto-encoders. In this paper, we present a new neural network architecture for model-free reinforcement learning. Our dueling network represents two separate estimators: one for the state value function and one for the state-dependent action advantage function. The main benefit of this factoring is to generalize learning across actions without imposing any change to the underlying reinforcement learning algorithm. Our results show that this architecture leads to better policy evaluation in the presence of many similar-valued actions. Moreover, the dueling architecture enables our RL agent to outperform the state-of-the-art on the Atari 2600 domain.},
  archiveprefix = {arXiv},
  eprint = {1511.06581},
  eprinttype = {arxiv},
  file = {C\:\\Users\\franc\\Zotero\\storage\\KJG2PATN\\Wang et al. - 2016 - Dueling Network Architectures for Deep Reinforceme.pdf},
  keywords = {Computer Science - Machine Learning},
  options = {useprefix=true},
  primaryclass = {cs}
}



@misc{gym,
  abstract = {OpenAI Gym is a toolkit for reinforcement learning research. It includes a
growing collection of benchmark problems that expose a common interface, and a
website where people can share their results and compare the performance of
algorithms. This whitepaper discusses the components of OpenAI Gym and the
design decisions that went into the software.},
  added-at = {2018-04-12T12:08:39.000+0200},
  author = {Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  biburl = {https://www.bibsonomy.org/bibtex/2cdc8f927d6c8657ea82951a09e34161a/achakraborty},
  description = {[1606.01540] OpenAI Gym},
  interhash = {cfd0ba0b44eda9a3ca67480dfbf823a0},
  intrahash = {cdc8f927d6c8657ea82951a09e34161a},
  keywords = {2016 arxiv paper reinforcement-learning},
  note = {cite arxiv:1606.01540},
  timestamp = {2018-04-12T12:08:39.000+0200},
  title = {OpenAI Gym},
  url = {http://arxiv.org/abs/1606.01540},
  year = 2016
}


@online{deepmind_atari,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  date = {2013-12-19},
  url = {http://arxiv.org/abs/1312.5602},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arXiv},
  eprint = {1312.5602},
  eprinttype = {arxiv},
  file = {C\:\\Users\\franc\\Zotero\\storage\\S36FW7V2\\Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;C\:\\Users\\franc\\Zotero\\storage\\FLQC8XMX\\1312.html},
  keywords = {Computer Science - Machine Learning},
  primaryclass = {cs}
}


@article{deepmind_nature,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  date = {2015-02},
  journaltitle = {Nature},
  volume = {518},
  pages = {529--533},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  url = {https://www.nature.com/articles/nature14236},
  urldate = {2021-06-26},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computer science Subject\_term\_id: computer-science},
  file = {C\:\\Users\\franc\\Zotero\\storage\\GZCZNVIY\\Mnih et al. - 2015 - Human-level control through deep reinforcement lea.pdf;C\:\\Users\\franc\\Zotero\\storage\\4D4FSCWQ\\nature14236.html},
  issue = {7540},
  langid = {english},
  number = {7540}
}




