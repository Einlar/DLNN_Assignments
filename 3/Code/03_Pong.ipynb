{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "1e37435746eaed37b8291da79a5f30415bfbec00a11726e7f2882afc7a4b1366"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 3 - Deep Reinforcement Learning\n",
    "## 3. Pong with pixels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoreload imported functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget \n",
    "#Or use %matplotlib notebook\n",
    "#I'm running jupyter inside of Visual Studio Code, so %matplotlib widget is needed for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl2latex import mpl2latex, latex_figsize\n",
    "from plotting import COLUMNWIDTH\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(filename='03_pong.log', encoding='utf-8', level=logging.INFO, filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Torch version: 1.9.0\nUsing \"cuda\" for training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "#Select device for training\n",
    "#device = \"cpu\" #For this very simple dataset it is actually faster\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") #Uncomment for GPU \n",
    "\n",
    "logging.info(f\"Using {device} for training\")\n",
    "print(f'Using \"{device}\" for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f4154e84850>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#---Rendering videos---#\n",
    "import gym\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "display = Display(visible=0, size=(700, 450))\n",
    "display.start()"
   ]
  },
  {
   "source": [
    "# Different environment\n",
    "3 pt: train a deep RL agent on a different Gym environment. You are free to choose whatever Gym environment you like from the available list, or even explore other simulation platforms: https://gym.openai.com/envs"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [INSTALL NOTE] Download ROMs from: https://github.com/openai/atari-py#roms\n",
    "#Then run this command:\n",
    "#python -m atari_py.import_roms <full path to ROM folder>\n",
    "#e.g. python -m atari_py.import_roms C:\\Users\\franc\\Documents\\GitHub\\DLNAssignments\\Homeworks\\3\\ROMS\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Run for 3648 frames\nFinal reward: -21.0\n"
     ]
    }
   ],
   "source": [
    "from utils import wrap_env\n",
    "\n",
    "#---Test the Atari environment---#\n",
    "env = gym.make('PongNoFrameskip-v4')\n",
    "# NOTE: PongNoFrameskip-v4 is chosen instead of Pong-v0, because the latter\n",
    "# by default adds randomness to actions, by repeating them for [2,4] frames,\n",
    "# which makes learning more difficult.\n",
    " \n",
    "env = wrap_env(env, video_callable=lambda episode_id: True)\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "n = 0\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    state, reward, done, info = env.step(env.action_space.sample())\n",
    "    env.render()\n",
    "    \n",
    "    n += 1\n",
    "    total_reward += reward\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Run for {n} frames\")\n",
    "print(f\"Final reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "class DuelingConvDQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_space_dim : tuple,\n",
    "                 action_space_dim : int):\n",
    "        \"\"\"Convolutional Neural Network for Reinforcement Learning of the\n",
    "        Cartpole environment using pixels.\n",
    "\n",
    "        Architecture is taken from the Atari paper by Deepmind:\n",
    "        https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "        \"Dueling networks\" are an improvement of RL shown in this paper: \n",
    "        https://arxiv.org/abs/1511.06581\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_space_dim : tuple of int\n",
    "            Dimensions of a state\n",
    "        action_space_dim : int\n",
    "            How many actions are available\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = state_space_dim\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(state_space_dim[0], 16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            # nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            # nn.ReLU()\n",
    "        )\n",
    "\n",
    "        num_features = functools.reduce(operator.mul, list(self.conv(torch.rand(1, *state_space_dim)).shape))\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(num_features, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space_dim) \n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(num_features, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x : \"torch.tensor\"):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1) #Flatten\n",
    "\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        return value + advantage - advantage.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(1, 84, 84)\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ec87e92e7894e1785de436bae1c0f1f"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "#---Wrappers---#\n",
    "import random\n",
    "import cv2 \n",
    "\n",
    "class PongPixels(gym.ObservationWrapper):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Resize Pong frames to 84x84, as it was done in the Atari paper by Deepmind:\n",
    "        https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        env : \"gym.Env\"\n",
    "            Instance of Pong environment\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.image_width = 84\n",
    "        self.image_height = 84\n",
    "        self.observation_space = gym.spaces.Box(low=0., high=1., shape=(1, self.image_height, self.image_width), dtype=np.float32)\n",
    "\n",
    "    def observation(self, frame):\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY) #convert to grayscale\n",
    "        frame = cv2.resize(frame, (self.image_width, self.image_height), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "        return np.array(frame[np.newaxis, :, :] / 255., dtype=np.float32) #shape (1, 84, 84)\n",
    "\n",
    "#---Print an image of what the network sees---#\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = PongPixels(env)\n",
    "\n",
    "state = env.reset()\n",
    "for i in range(161): #Step a bit\n",
    "    state, reward, done, info = env.step(0)\n",
    "\n",
    "print(state.shape)\n",
    "fig = plt.figure()\n",
    "plt.imshow(state[0], cmap='gist_gray')\n",
    "plt.show()\n",
    "\n",
    "#---Save a video of an episode---#\n",
    "state = env.reset()\n",
    "states = [state]\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    state, reward, done, info = env.step(random.randint(0, 5))\n",
    "    states.append(state)\n",
    "\n",
    "fps = 25\n",
    "out = cv2.VideoWriter('cnn_pong.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (84, 84), False)\n",
    "for state in states:\n",
    "    out.write(np.array(state[0] * 255, dtype=np.uint8))\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndEpisodeOnLosingLife(gym.Wrapper):\n",
    "    def __init__(self, env : \"gym.Env\"):\n",
    "        \"\"\"\n",
    "        An episode is terminated when a life is lost, and not on game over\n",
    "        (all lives lost). This makes episodes shorter, and so learning faster.\n",
    "        Moreover, it helps the RL agent understand that \"losing a life = bad\".\n",
    "\n",
    "        It was originally done in the Nature paper by DeepMind researchers:\n",
    "        https://www.nature.com/articles/nature14236\n",
    "\n",
    "        However, it's usefulness is debated, for instance in https://arxiv.org/abs/1709.06009 it is recommended against. In this homework, it is mainly done for \n",
    "        performance reasons (shorter episodes).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        env : \"gym.Env\"\n",
    "            Pong Environment\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env)\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "        self.previous_lives = 0\n",
    "        self.final_done = True\n",
    "\n",
    "    def get_lives(self):\n",
    "        return self.env.unwrapped.ale.lives() #How many lives the player has\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, info = self.env.step(action)\n",
    "        self.final_done = done\n",
    "\n",
    "        current_lives = self.get_lives()\n",
    "        if current_lives < self.previous_lives and current_lives > 0: #If the player has lost a life, but there are other lives left\n",
    "            done = True #Terminate the episode\n",
    "\n",
    "        self.previous_lives = current_lives\n",
    "\n",
    "        return state, reward, done, info\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        if self.final_done: #Reset only on losing the last life\n",
    "            state = self.env.reset(**kwargs)\n",
    "        else:\n",
    "            state, reward, done, info = self.env.step(0) #Skip an action\n",
    "            #(The game has a reset frame after losing a life)\n",
    "        \n",
    "        self.previous_lives = self.get_lives()\n",
    "\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrames(gym.Wrapper):\n",
    "    def __init__(self, env : \"gym.Env\", skip : int = 4):\n",
    "        \"\"\"\n",
    "        Each action is repeated for `skip` frames, and only the `skip`-th frame\n",
    "        is returned as an observation. Reward is summed over the skipped frames.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.skip = skip\n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self.skip):\n",
    "            state, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return state, total_reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DeepQLearner\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "#---Make environment---#\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = SkipFrames(env, 4)\n",
    "env = EndEpisodeOnLosingLife(env)\n",
    "env = PongPixels(env)\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 30000\n",
    "\n",
    "#---Define epsilon decay---#\n",
    "def epsilon_per_frame(xs):\n",
    "    return epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1 * xs / epsilon_decay)\n",
    "\n",
    "num_frames = 1000000\n",
    "temperature = epsilon_per_frame(np.arange(num_frames))\n",
    "\n",
    "hyper_parameters = {\n",
    "    \"gamma\" : .99,\n",
    "    \"replay_memory_capacity\" : 100000,\n",
    "    \"min_samples_for_training\" : 10000,\n",
    "    \"batch_size\" : 32,\n",
    "    \"target_net_update_steps\" : 1000,\n",
    "    \"loss_function\" : \"SmoothL1Loss\",\n",
    "    \"temperature_policy\" : temperature,\n",
    "    \"steps_per_epoch\" : 10000,\n",
    "    \"learning_rate\" : 1e-4\n",
    "    \n",
    "}\n",
    "# RL_net = DeepQLearner(env=env, \n",
    "#                       Network=DuelingConvDQN, \n",
    "#                       hyper_parameters=hyper_parameters,\n",
    "#                       chosen_policy=\"epsilon-greedy\",\n",
    "#                       count_steps=False,\n",
    "#                       update_temp_every_frame=True,\n",
    "#                       update_target_every_frame=True,\n",
    "#                       save_video_every_n_episodes=1,\n",
    "#                       ) \n",
    "# [UNCOMMENT] the previous line if you want to initialize the model (for re-training it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import NotebookProgressBar\n",
    "\n",
    "bar = NotebookProgressBar()\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_steps=num_frames, callbacks=[bar], gradient_clip_val=2) \n",
    "\n",
    "#trainer.fit(RL_net) \n",
    "# [UNCOMMENT] the previous line to re-run training. Otherwise, the next cell will load a saved checkpoint.\n",
    "# During training, scores of episodes are saved in `03_pong.log`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "save = False\n",
    "# [SET] save to True if you want to save a checkpoint for the trained model\n",
    "# (unnecessary if training is not re-executed)\n",
    "\n",
    "if save:\n",
    "    all_info = deepcopy(RL_net.hyper_parameters)\n",
    "    all_info[\"score\"] = deepcopy(RL_net.episode_history)\n",
    "    all_info[\"temp\"]  = deepcopy(RL_net.temp_history)\n",
    "\n",
    "    DATE_FMT = \"%d_%m_%y-%Hh%Mm%S\"\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(DATE_FMT) #Current time\n",
    "\n",
    "    os.makedirs(\"SavedModels/3/\", exist_ok=True)\n",
    "\n",
    "    with open(f\"SavedModels/3/{date}.result\", 'wb') as file:\n",
    "        file.write(pickle.dumps(all_info))\n",
    "\n",
    "    trainer.save_checkpoint(f\"SavedModels/3/{date}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f029387282624608aec922f11c4ccf33"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a4cddc65b784071a8cccb376a2f404a"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "#---Plot learning curves---#\n",
    "\n",
    "def moving_average(x : \"np.ndarray\", w_size : int):\n",
    "    \"\"\"Compute the rolling average of a 1D array `x`, averaging the values\n",
    "    within a window of length `w_size'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xs : np.ndarray\n",
    "        Indices of the original array representing the \"centers\" of convolved points\n",
    "    ys : np.ndarray\n",
    "        Convolved points\n",
    "    \"\"\"\n",
    "\n",
    "    return (np.arange(w_size // 2, len(x) - (w_size - w_size // 2) + 1), \n",
    "           np.convolve(x, np.ones(w_size), 'valid') / w_size)\n",
    "\n",
    "for file in glob(\"SavedModels/3/*.result\"):\n",
    "    with open(file, 'rb') as f:\n",
    "        new_var = pickle.loads(f.read()) #works \n",
    "\n",
    "    with mpl2latex(True):\n",
    "        fig, ax1 = plt.subplots(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.plot(new_var['score'], color=color, label='Score (Raw)', alpha=.4)\n",
    "        ax1.plot(*moving_average(new_var['score'], 10), '--', color=color, label='Score (Avg. 10)')\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Score', color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Epsilon', color=color, rotation=270, labelpad=15)\n",
    "        ax2.plot(new_var['temp'], color=color, label='Epsilon')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax1.set_title(\"Pong-Pixels - Training\")\n",
    "\n",
    "        textstr = f\"\"\"replay\\_capacity: {new_var['replay_memory_capacity']}\n",
    "gamma: {new_var['gamma']}\n",
    "lr: {new_var['learning_rate']}\n",
    "batch: {new_var['batch_size']}\n",
    "update\\_every: {new_var['target_net_update_steps']}\"\"\"\n",
    "\n",
    "        props = dict(boxstyle='round, pad=.3', facecolor='gray', alpha=.1)\n",
    "        ax1.text(0.65, 0.3, textstr, transform=ax1.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        ax1.legend(loc=(0.1, 0.7))\n",
    "        ax2.legend(loc=(0.1, 0.9))\n",
    "\n",
    "        filename = os.path.splitext(os.path.basename(file))[0]\n",
    "        fig.savefig(f\"Plots/3/{filename}.pdf\", transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/einlar/miniconda3/envs/torch/lib/python3.9/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Trying to monitor an environment which has no 'spec' set. This usually means you did not create it via 'gym.make', and is recommended only for advanced users.\u001b[0m\n  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "757cc08eae354924a14dcfd39659e2aa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83dc7e6c31d54fd1afc9f8af0d5d026f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPISODE 1 - FINAL SCORE: 20.0\n",
      "EPISODE 2 - FINAL SCORE: 20.0\n",
      "EPISODE 3 - FINAL SCORE: 20.0\n",
      "EPISODE 4 - FINAL SCORE: 20.0\n",
      "EPISODE 5 - FINAL SCORE: 20.0\n",
      "EPISODE 6 - FINAL SCORE: 20.0\n",
      "EPISODE 7 - FINAL SCORE: 20.0\n",
      "EPISODE 8 - FINAL SCORE: 20.0\n",
      "EPISODE 9 - FINAL SCORE: 20.0\n",
      "EPISODE 10 - FINAL SCORE: 20.0\n",
      "Average score (10 trials): 20.000\n"
     ]
    }
   ],
   "source": [
    "#---Test---#\n",
    "\n",
    "from model import DeepQLearner\n",
    "from utils import wrap_env\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "#---Test the final training---#\n",
    "\n",
    "model = DeepQLearner.load_from_checkpoint(\"SavedModels/3/25_06_21-22h54m13.ckpt\", min_samples_for_training=0) \n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make(\"PongNoFrameskip-v4\")\n",
    "env = SkipFrames(env, 4)\n",
    "#env = EndEpisodeOnLosingLife(env)\n",
    "env = PongPixels(env)\n",
    "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n",
    "\n",
    "scores = []\n",
    "states = []\n",
    "model.eval()\n",
    "\n",
    "for num_episode in tqdm(range(10)): \n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "      states.append(state)\n",
    "\n",
    "      with torch.no_grad():\n",
    "        action = int(model.policy_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).argmax())\n",
    "      \n",
    "      next_state, reward, done, info = env.step(action)\n",
    "\n",
    "      env.render()\n",
    "      score += reward \n",
    "      state = next_state\n",
    "\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "    scores.append(score)\n",
    "\n",
    "    # Save a video of what the network sees\n",
    "    size = states[0][0].shape\n",
    "    fps = 25\n",
    "    out = cv2.VideoWriter(f'video/cnn_{num_episode}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (size[1], size[0]), False)\n",
    "    for state in states:\n",
    "        out.write(np.array(state[-1] * 255, dtype=np.uint8))\n",
    "    out.release()\n",
    "  \n",
    "print(f\"Average score (10 trials): {np.mean(scores):.3f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}