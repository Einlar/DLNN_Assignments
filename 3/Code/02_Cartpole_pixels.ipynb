{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('torch': conda)"
  },
  "interpreter": {
   "hash": "1e37435746eaed37b8291da79a5f30415bfbec00a11726e7f2882afc7a4b1366"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Homework 3 - Deep Reinforcement Learning\n",
    "## 2. Cartpole with pixels"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Autoreload imported functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib widget \n",
    "#Or use %matplotlib notebook\n",
    "#I'm running jupyter inside of Visual Studio Code, so %matplotlib widget is needed for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl2latex import mpl2latex, latex_figsize\n",
    "from plotting import COLUMNWIDTH\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import logging \n",
    "logging.basicConfig(filename='02_Cartpole_pixels.log', encoding='utf-8', level=logging.INFO, filemode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Torch version: 1.9.0\nUsing \"cuda\" for training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
    "import torchvision\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "\n",
    "#Select device for training\n",
    "#device = \"cpu\" #For this very simple dataset it is actually faster\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") #Uncomment for GPU \n",
    "\n",
    "logging.info(f\"Using {device} for training\")\n",
    "print(f'Using \"{device}\" for training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7f45b081dfa0>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "#---Rendering videos---#\n",
    "import gym\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "import os\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "from pyvirtualdisplay import Display\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "display = Display(visible=0, size=(700, 450))\n",
    "display.start()"
   ]
  },
  {
   "source": [
    "# Screen Pixels\n",
    "\n",
    "3 pt: extend the notebook used in Lab 07, in order to learn to control the CartPole environment using directly the screen pixels, rather than the compact state representation used during the Lab (cart position, cart velocity, pole angle, pole angular velocity). This will require to change the “observation_space”."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total steps: 32\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from utils import wrap_env, CartpolePixels\n",
    "\n",
    "#---Test the CartPole with Pixels and a random policy---#\n",
    "env = gym.make(\"CartPole-v0\")\n",
    "env = CartpolePixels(env)\n",
    "env = wrap_env(env)\n",
    "\n",
    "state = env.reset()\n",
    "states = [state]\n",
    "\n",
    "done=False\n",
    "i = 0\n",
    "\n",
    "while not done:\n",
    "    state, reward, done, info = env.step(random.choice([0, 1]))\n",
    "    i += 1\n",
    "    states.append(state)\n",
    "    #print(env.unwrapped.state[0])\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(f\"Total steps: {i}\")\n",
    "\n",
    "#---Save a video of what the network sees---#\n",
    "size = states[0].shape\n",
    "fps = 5\n",
    "out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (size[1], size[0]), False)\n",
    "for state in states:\n",
    "    out.write(np.array(state * 255, dtype=np.uint8))\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import operator\n",
    "\n",
    "class DuelingConvDQN(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 state_space_dim : tuple,\n",
    "                 action_space_dim : int):\n",
    "\n",
    "        \"\"\"Convolutional Neural Network for Reinforcement Learning of the\n",
    "        Cartpole environment using pixels.\n",
    "\n",
    "        Architecture is taken from the Atari paper by Deepmind:\n",
    "        https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "\n",
    "        \"Dueling networks\" are an improvement of RL shown in this paper: \n",
    "        https://arxiv.org/abs/1511.06581\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state_space_dim : tuple of int\n",
    "            Dimensions of a state\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        self.input_shape = state_space_dim\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(state_space_dim[0], 16, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        num_features = functools.reduce(operator.mul, list(self.conv(torch.rand(1, *state_space_dim)).shape))\n",
    "\n",
    "\n",
    "        self.advantage = nn.Sequential(\n",
    "            nn.Linear(num_features, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_space_dim) \n",
    "        )\n",
    "\n",
    "        self.value = nn.Sequential(\n",
    "            nn.Linear(num_features, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1) \n",
    "        )\n",
    "\n",
    "    def forward(self, x : \"torch.tensor\"):\n",
    "        x = self.conv(x)\n",
    "        x = x.view(x.size(0), -1) #Flatten\n",
    "\n",
    "        advantage = self.advantage(x)\n",
    "        value = self.value(x)\n",
    "\n",
    "        return value + advantage - advantage.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Temperature/Epsilon---#\n",
    "\n",
    "epsilon_start = 1.0\n",
    "epsilon_final = 0.01\n",
    "epsilon_decay = 2500 #1300\n",
    "\n",
    "def epsilon_per_frame(xs):\n",
    "    return epsilon_final + (epsilon_start - epsilon_final) * np.exp(-1 * xs / epsilon_decay)\n",
    "      \n",
    "\n",
    "num_frames = 100000 \n",
    "temperature = epsilon_per_frame(np.arange(num_frames))\n",
    "#Here I use \"temperature\" for both epsilon and temperature, since there is the option of using either \"epsilon-greedy\" or \"softmax\" as decision policies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DeepQLearner\n",
    "from gym.wrappers import FrameStack\n",
    "from utils import CartpolePixels\n",
    "\n",
    "def preprocess_env(env):\n",
    "    \"\"\"Preprocess a CartPole env\"\"\"\n",
    "\n",
    "    env = CartpolePixels(env) #Return pixels instead of the state\n",
    "    env = FrameStack(env, 4)  #Stack 4 frames in each observation,\n",
    "    # so that the network can infer velocity/acceleration\n",
    "\n",
    "    return env\n",
    "\n",
    "hyper_parameters = {\n",
    "    \"gamma\" : .99,\n",
    "    \"replay_memory_capacity\" : 10000, \n",
    "    \"min_samples_for_training\" : 1000, \n",
    "    \"batch_size\" : 32,\n",
    "    \"target_net_update_steps\" : 300, \n",
    "    \"loss_function\" : \"SmoothL1Loss\",\n",
    "    \"temperature_policy\" : temperature,\n",
    "    \"steps_per_epoch\" : 1000,\n",
    "    \"learning_rate\" : 1e-4\n",
    "    \n",
    "}\n",
    "# RL_net = DeepQLearner(env=\"CartPole-v1\",\n",
    "#                       preprocess_env=preprocess_env,\n",
    "#                       Network=DuelingConvDQN, \n",
    "#                       hyper_parameters=hyper_parameters,\n",
    "#                       chosen_policy=\"epsilon-greedy\",\n",
    "#                       count_steps=True,\n",
    "#                       update_temp_every_frame=True,\n",
    "#                       update_target_every_frame=True,\n",
    "#                       save_video_every_n_episodes=25,\n",
    "#                       add_to_reward = lambda state, reward : -1 * np.abs(state[0])\n",
    "#                       ) \n",
    "# [UNCOMMENT] the previous line if you want to initialize the model (for re-training it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Callback\n",
    "\n",
    "class LearningRateAdjust(Callback):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Adjusts the learning rate based on the current value of the reward.\n",
    "        Higher reward = lower learning rate, which helps avoiding \"catastrophic forgetting\".\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_episodes = 0\n",
    "        \n",
    "    def on_batch_start(self, trainer, pl_module):\n",
    "        lr = 1e-4\n",
    "\n",
    "        if pl_module.n_episodes > self.n_episodes: #Adjust just after the end of an episode\n",
    "            reward = pl_module.total_reward\n",
    "            if reward > 100:\n",
    "                lr = .5e-4\n",
    "            if reward > 200:\n",
    "                lr = 1e-5\n",
    "            if reward > 300:\n",
    "                lr = 1e-6\n",
    "            if reward > 400:\n",
    "                lr = 1e-7\n",
    "\n",
    "            for param_group in pl_module.optimizers().param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from callbacks import NotebookProgressBar, StopAfterNEpisodes\n",
    "\n",
    "\n",
    "bar = NotebookProgressBar()\n",
    "lr_adj = LearningRateAdjust()\n",
    "stop = StopAfterNEpisodes(1000)\n",
    "\n",
    "trainer = pl.Trainer(gpus=1, max_steps=num_frames, callbacks=[bar, stop, lr_adj], gradient_clip_val=2) \n",
    "\n",
    "#trainer.fit(RL_net) \n",
    "# [UNCOMMENT] the previous line to re-run training. Otherwise, the next cell will load a saved checkpoint.\n",
    "# During training, scores of episodes are saved in `02_Cartpole_pixels.log`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---Save the model and the learning stats---#\n",
    "\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from datetime import datetime\n",
    "\n",
    "save = False\n",
    "# [SET] save to True if you want to save a checkpoint for the trained model\n",
    "# (unnecessary if training is not re-executed)\n",
    "\n",
    "if save:\n",
    "    all_info = deepcopy(RL_net.hyper_parameters)\n",
    "    all_info[\"score\"] = deepcopy(RL_net.episode_history)\n",
    "    all_info[\"temp\"]  = deepcopy(RL_net.temp_history)\n",
    "\n",
    "    DATE_FMT = \"%d_%m_%y-%Hh%Mm%S\"\n",
    "    now = datetime.now()\n",
    "    date = now.strftime(DATE_FMT) #Current time\n",
    "\n",
    "    os.makedirs(\"SavedModels/2\", exist_ok=True)\n",
    "\n",
    "    with open(f\"SavedModels/2/{date}.result\", 'wb') as file:\n",
    "        file.write(pickle.dumps(all_info))\n",
    "\n",
    "    trainer.save_checkpoint(f\"SavedModels/2/{date}.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9cca22e0b4dc4bcca43baadc0858e7f5"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07af5f0cdbb04e059e7dec2dcad8d5f0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7166005fc904bf88bfef6107413c9c8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6903321e72a14237a00853b1c7c7ab9a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "832f996a04f9435ea864a0941fd54b1d"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "from glob import glob\n",
    "import pickle\n",
    "\n",
    "#---Plot learning curves---#\n",
    "\n",
    "def moving_average(x : \"np.ndarray\", w_size : int):\n",
    "    \"\"\"Compute the rolling average of a 1D array `x`, averaging the values\n",
    "    within a window of length `w_size'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    xs : np.ndarray\n",
    "        Indices of the original array representing the \"centers\" of convolved points\n",
    "    ys : np.ndarray\n",
    "        Convolved points\n",
    "    \"\"\"\n",
    "\n",
    "    return (np.arange(w_size // 2, len(x) - (w_size - w_size // 2) + 1), \n",
    "           np.convolve(x, np.ones(w_size), 'valid') / w_size)\n",
    "\n",
    "for file in glob(\"SavedModels/2/*.result\"):\n",
    "    with open(file, 'rb') as f:\n",
    "        new_var = pickle.loads(f.read()) #works \n",
    "\n",
    "    with mpl2latex(True):\n",
    "        fig, ax1 = plt.subplots(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.plot(new_var['score'], color=color, label='Score (Raw)', alpha=.4)\n",
    "        ax1.plot(*moving_average(new_var['score'], 10), '--', color=color, label='Score (Avg. 10)')\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Score', color=color)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax2 = ax1.twinx()\n",
    "        \n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Epsilon', color=color, rotation=270, labelpad=15)\n",
    "        ax2.plot(new_var['temp'], color=color, label='Epsilon')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "        ax1.set_title(\"CartPole-v1-Pixels - Training\")\n",
    "\n",
    "        textstr = f\"\"\"replay\\_capacity: {new_var['replay_memory_capacity']}\n",
    "gamma: {new_var['gamma']}\n",
    "lr: {new_var['learning_rate']}\n",
    "batch: {new_var['batch_size']}\n",
    "update\\_every: {new_var['target_net_update_steps']}\"\"\"\n",
    "\n",
    "        props = dict(boxstyle='round, pad=.3', facecolor='gray', alpha=.1)\n",
    "        ax1.text(0.35, 0.95, textstr, transform=ax1.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
    "        fig.tight_layout()\n",
    "\n",
    "        ax1.legend(loc=(0.05, 0.1))\n",
    "        ax2.legend(loc=(0.05, 0.3))\n",
    "\n",
    "        filename = os.path.splitext(os.path.basename(file))[0]\n",
    "        fig.savefig(f\"Plots/2/{filename}.pdf\", transparent=True, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c24f0f8f586c4d7583508f0727d92667"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "058500c12afd4e46af50386a8d551b1b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EPISODE 1 - FINAL SCORE: 500.0\n",
      "EPISODE 2 - FINAL SCORE: 392.0\n",
      "EPISODE 3 - FINAL SCORE: 500.0\n",
      "EPISODE 4 - FINAL SCORE: 500.0\n",
      "EPISODE 5 - FINAL SCORE: 500.0\n",
      "EPISODE 6 - FINAL SCORE: 500.0\n",
      "EPISODE 7 - FINAL SCORE: 408.0\n",
      "EPISODE 8 - FINAL SCORE: 500.0\n",
      "EPISODE 9 - FINAL SCORE: 454.0\n",
      "EPISODE 10 - FINAL SCORE: 500.0\n",
      "Average score (10 trials): 475.400\n"
     ]
    }
   ],
   "source": [
    "from model import DeepQLearner\n",
    "from utils import wrap_env, CartpolePixels\n",
    "from gym.wrappers import FrameStack\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "\n",
    "def preprocess_env(env):\n",
    "    \"\"\"Preprocess a CartPole env\"\"\"\n",
    "\n",
    "    env = CartpolePixels(env) #Return pixels instead of the state\n",
    "    env = FrameStack(env, 4)  #Stack 4 frames in each observation,\n",
    "    # so that the network can infer velocity/acceleration\n",
    "\n",
    "    return env\n",
    "\n",
    "#---Test the final training---#\n",
    "\n",
    "model = DeepQLearner.load_from_checkpoint(\"SavedModels/2/25_06_21-02h24m16.ckpt\", min_samples_for_training=0) \n",
    "\n",
    "# Initialize the Gym environment\n",
    "env = gym.make('CartPole-v1') \n",
    "env = CartpolePixels(env)\n",
    "env = FrameStack(env, 4)\n",
    "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
    "\n",
    "# This is for creating the output video in Colab, not required outside Colab\n",
    "env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n",
    "\n",
    "scores = []\n",
    "states = []\n",
    "model.eval()\n",
    "# Let's try for a total of 10 episodes\n",
    "for num_episode in tqdm(range(10)): \n",
    "    state = env.reset()\n",
    "    score = 0\n",
    "    done = False\n",
    "  \n",
    "    while not done:\n",
    "      states.append(state)\n",
    "      with torch.no_grad():\n",
    "        action = int(model.policy_net(torch.tensor(state, dtype=torch.float32).unsqueeze(0)).argmax())\n",
    "\n",
    "      next_state, reward, done, info = env.step(action)\n",
    "      env.render()\n",
    "      score += reward \n",
    "      state = next_state\n",
    "\n",
    "    # Print the final score\n",
    "    scores.append(score)\n",
    "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
    "\n",
    "    # Save a video of what the network sees\n",
    "    size = states[0][0].shape\n",
    "    fps = 25\n",
    "    out = cv2.VideoWriter(f'video/cnn_{num_episode}.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (size[1], size[0]), False)\n",
    "    for state in states:\n",
    "        out.write(np.array(state[-1] * 255, dtype=np.uint8))\n",
    "    out.release()\n",
    "\n",
    "env.close()\n",
    "print(f\"Average score (10 trials): {np.mean(scores):.3f}\")"
   ]
  }
 ]
}