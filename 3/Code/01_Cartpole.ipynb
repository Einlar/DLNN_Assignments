{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nndl_2020__homework_3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.5 64-bit ('torch': conda)"
    },
    "interpreter": {
      "hash": "1e37435746eaed37b8291da79a5f30415bfbec00a11726e7f2882afc7a4b1366"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYncZVOyF8Rb"
      },
      "source": [
        "#NEURAL NETWORKS AND DEEP LEARNING\n",
        "> M.Sc. ICT FOR LIFE AND HEALTH\n",
        "> \n",
        "> Department of Information Engineering\n",
        "\n",
        "> M.Sc. COMPUTER ENGINEERING\n",
        ">\n",
        "> Department of Information Engineering\n",
        "\n",
        "> M.Sc. AUTOMATION ENGINEERING\n",
        ">\n",
        "> Department of Information Engineering\n",
        " \n",
        "> M.Sc. PHYSICS OF DATA\n",
        ">\n",
        "> Department of Physics and Astronomy\n",
        " \n",
        "> M.Sc. COGNITIVE NEUROSCIENCE AND CLINICAL NEUROPSYCHOLOGY\n",
        ">\n",
        "> Department of General Psychology\n",
        "\n",
        "---\n",
        "A.A. 2020/21 (6 CFU) - Dr. Alberto Testolin, Dr. Matteo Gadaleta\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgHJFDwYvgHg"
      },
      "source": [
        "# Homework 3 - Deep Reinforcement Learning\n",
        "# 1. Cartpole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNENm7RHGFMd"
      },
      "source": [
        "## General overview\n",
        "In this homework you will learn how to implement and test neural network models for solving reinforcement learning problems. The basic tasks for the homework will require to implement some extensions to the code that you have seen in the Lab. More advanced tasks will require to train and test your learning agent on a different environment. Given the higher computational complexity of RL, in this homework you don’t need to tune learning hyperparameters using search procedures and cross-validation; however, you are encouraged to play with model hyperparameters in order to find a satisfactory configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZGjMokMvk2h"
      },
      "source": [
        "\n",
        "## Technical notes\n",
        "The homework should be implemented in Python using the PyTorch framework. The student can explore additional libraries and tools to implement the models; however, please make sure you understand the code you are writing because during the exam you might receive specific questions related to your implementation. The entire source code required to run the homework must be uploaded as a compressed archive in a Moodle section dedicated to the homework. If your code will be entirely included in a single Python notebook, just upload the notebook file.\n",
        "\n",
        "As an example of more advanced libraries that can be used to implement deep RL agents, you can check this website:\n",
        "\n",
        "https://stable-baselines.readthedocs.io/en/master/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYZUBEMCvlpB"
      },
      "source": [
        "\n",
        "## Final report\n",
        "Along with the source code, you must separately upload a PDF file containing a brief report of your homework. The report should include a brief Introduction on which you explain the homework goals and the main implementation strategies you choose, a brief Method section where you describe your model architectures and hyperparameters, and a Result section where you present the simulation results. Total length must not exceed 6 pages, though you can include additional tables and figures in a final Appendix (optional). Given the dynamical nature of RL problems, you can explore more sophisticated media for showing the results of your model (e.g., animated GIFs or short movies).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RchPc7G6vmRB"
      },
      "source": [
        "\n",
        "## Grade\n",
        "The maximum grade for this homework will be **8 points**. Points will be assigned based on the correct implementation of the following items:\n",
        "*\t2 pt: extend the notebook used in Lab 07, in order to study how the exploration profile (either using eps-greedy or softmax) impacts the learning curve. Try to tune the model hyperparameters or tweak the reward function in order to speed-up learning convergence (i.e., reach the same accuracy with fewer training episodes).\n",
        "*\t3 pt: extend the notebook used in Lab 07, in order to learn to control the CartPole environment using directly the screen pixels, rather than the compact state representation used during the Lab (cart position, cart velocity, pole angle, pole angular velocity). This will require to change the “observation_space”.\n",
        "*\t3 pt: train a deep RL agent on a different Gym environment. You are free to choose whatever Gym environment you like from the available list, or even explore other simulation platforms:\n",
        "https://gym.openai.com/envs \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYE6Cjhgvm3B"
      },
      "source": [
        "\n",
        "## Deadline\n",
        "The complete homework (source code + report) must be submitted through Moodle at least 10 days before the chosen exam date."
      ]
    },
    {
      "source": [
        "The following **Ubuntu** packages are needed: `ffmpeg`, `python-opengl`, `xvfb`"
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "#Install all the required packages with the correct versions in the current environment.\n",
        "#Note: this notebook has been run with Python 3.9.5 on a 64-bit **Ubuntu 20.04** machine, with a AMD Ryzen 7 1700 8-core CPU, GTX 970 GPU, and 32GB of DDR4 RAM. \n",
        "\n",
        "#[UNCOMMENT] the following line to install the packages.\n",
        "# !{sys.executable} -m pip install numpy~=1.20.1 pandas~=1.2.3 matplotlib~=3.3.4 hiplot~=0.1.24 scipy~=1.6.0 tqdm~=4.59.0 torch~=1.8.0 torchvision~=0.9.0 optuna~=2.7.0 pytorch_lightning~=1.3.4 torchmetrics~=0.3.2 gym~=0.18.3 atari_py~=0.2.9 ipympl~=0.7.0 pyvirtualdisplay~=2.2 piglet~=1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Autoreload imported functions\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "%matplotlib widget \n",
        "#Or use %matplotlib notebook\n",
        "#I'm running jupyter inside of Visual Studio Code, so %matplotlib widget is needed for me."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl2latex import mpl2latex, latex_figsize\n",
        "from plotting import COLUMNWIDTH\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "import logging \n",
        "logging.basicConfig(filename='01_Cartpole.log', encoding='utf-8', level=logging.INFO, filemode='w')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch version: 1.9.0\nUsing \"cuda\" for training\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import torchvision\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "print(\"Torch version:\", torch.__version__)\n",
        "\n",
        "#Select device for training\n",
        "#device = \"cpu\" #For this very simple dataset it is actually faster\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\") #Uncomment for GPU \n",
        "\n",
        "logging.info(f\"Using {device} for training\")\n",
        "print(f'Using \"{device}\" for training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fa5a7f0b3d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import gym\n",
        "import os\n",
        "from glob import glob\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display = Display(visible=0, size=(700, 450))\n",
        "display.start()"
      ]
    },
    {
      "source": [
        "# Exploration Profile\n",
        "\n",
        "2 pt: extend the notebook used in Lab 07, in order to study how the exploration profile (either using eps-greedy or softmax) impacts the learning curve. Try to tune the model hyperparameters or tweak the reward function in order to speed-up learning convergence (i.e., reach the same accuracy with fewer training episodes)."
      ],
      "cell_type": "markdown",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulation run for 30 steps\n"
          ]
        }
      ],
      "source": [
        "#Quick test of all the modules\n",
        "from data import ReplayMemory\n",
        "from model import DQN\n",
        "from agent import Agent\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "memory = ReplayMemory(capacity=10)\n",
        "net = DQN(state_space_dim=(4,), action_space_dim=2)\n",
        "\n",
        "agent = Agent(env, memory)\n",
        "\n",
        "done = False\n",
        "i = 0\n",
        "while not done:\n",
        "    reward, done = agent.play_step(net)\n",
        "    logging.debug(agent.state, done)\n",
        "    i += 1\n",
        "\n",
        "print(f\"Simulation run for {i} steps\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from model import DeepQLearner, DQN\n",
        "from callbacks import NotebookProgressBar, LearningRateAdjust, StopAfterNEpisodes\n",
        "\n",
        "\n",
        "bar = NotebookProgressBar()\n",
        "lr_adj = LearningRateAdjust()\n",
        "stop = StopAfterNEpisodes(1000)\n",
        "\n",
        "add_to_reward = lambda state, reward : -np.abs(state[0])\n",
        "\n",
        "# RL_net = DeepQLearner(env=\"CartPole-v1\", Network=DQN, reach_zero_temperature_after_n_episodes=400, batch_size=256, gamma=.98, target_net_update_steps=300, learning_rate=1e-1, min_samples_for_training=10000, add_to_reward=add_to_reward, update_target_every_frame=True) \n",
        "# [UNCOMMENT] the previous line to initialize the model to be trained. This involves stepping through 10k frames, which takes a bit of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "tags": []
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/einlar/miniconda3/envs/torch/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: GPU available but not used. Set the gpus flag in your trainer `Trainer(gpus=1)` or script `--gpus=1`.\n  warnings.warn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(gpus=0, max_epochs=1000000, callbacks=[bar, stop, lr_adj], gradient_clip_val=2) \n",
        "#max_epochs is a very large number, since the callback \"StopAfterNEpisodes\" is used to stop training\n",
        "\n",
        "# trainer.fit(RL_net) \n",
        "# [UNCOMMENT] the previous line to re-run training. Otherwise, the next cell will load a saved checkpoint.\n",
        "# During training, scores of episodes are saved in `01_Cartpole.log`. \n",
        "\n",
        "# NOTE: I encountered a weird error \"CUDA error: CUBLAS_STATUS_EXECUTION_FAILED\", which I solved by reinstalling PyTorch with conda (previously it was installed with pip).\n",
        "# The error is probably due to the specific environment used for running the code, so I'll leave this note here in case something similar happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#---Save the model and the learning stats---#\n",
        "\n",
        "import pickle\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "save = False\n",
        "# [SET] save to True to save the previously trained model\n",
        "# (not necessary if training is not re-executed)\n",
        "if save:\n",
        "    all_info = deepcopy(RL_net.hyper_parameters)\n",
        "    all_info[\"score\"] = deepcopy(RL_net.episode_history)\n",
        "    all_info[\"temp\"]  = deepcopy(RL_net.temp_history)\n",
        "\n",
        "    DATE_FMT = \"%d_%m_%y-%Hh%Mm%S\"\n",
        "    now = datetime.now()\n",
        "    date = now.strftime(DATE_FMT) #Current time\n",
        "\n",
        "    os.makedirs(\"SavedModels/1\", exist_ok=True)\n",
        "\n",
        "    with open(f\"SavedModels/1/{date}.result\", 'wb') as file:\n",
        "        file.write(pickle.dumps(all_info))\n",
        "\n",
        "    trainer.save_checkpoint(f\"SavedModels/1/{date}.ckpt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e796e38f735402092c78ba94fa49c1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef4d13526b0e4e6099556c131e32f7ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0b04735cccd4a5599b1221114067ada"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f0eabfea42c4eb1be3e5f46a4ffe8e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a092f2ff62884277b4f879ce12faba02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "671a2467350e4602960e73d23ecca648"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fe4abfe8fedb48d1949aacef2049dde3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab5b574740d74dffb42970d1e9757619"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c51a4a827eca45e69deac089b96d0058"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52353a9ae7f6430e9311a2fe2a226cdb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from glob import glob\n",
        "import pickle\n",
        "\n",
        "def moving_average(x : \"np.ndarray\", w_size : int):\n",
        "    \"\"\"Compute the rolling average of a 1D array `x`, averaging the values\n",
        "    within a window of length `w_size'.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    xs : np.ndarray\n",
        "        Indices of the original array representing the \"centers\" of convolved points\n",
        "    ys : np.ndarray\n",
        "        Convolved points\n",
        "    \"\"\"\n",
        "\n",
        "    return (np.arange(w_size // 2, len(x) - (w_size - w_size // 2) + 1), \n",
        "           np.convolve(x, np.ones(w_size), 'valid') / w_size)\n",
        "\n",
        "#---Plot the learning curve of all the attempted trials---#\n",
        "\n",
        "for file in glob(\"SavedModels/1/*.result\"):\n",
        "    with open(file, 'rb') as f:\n",
        "        new_var = pickle.loads(f.read()) #works \n",
        "\n",
        "    with mpl2latex(True):\n",
        "        fig, ax1 = plt.subplots(figsize=latex_figsize(wf=1., columnwidth=COLUMNWIDTH))\n",
        "\n",
        "        color = 'tab:red'\n",
        "        ax1.plot(new_var['score'], color=color, label='Score (Raw)', alpha=.4)\n",
        "        ax1.plot(*moving_average(new_var['score'], 10), '--', color=color, label='Score (Avg. 10)')\n",
        "        ax1.set_xlabel('Episode')\n",
        "        ax1.set_ylabel('Score', color=color)\n",
        "        ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        \n",
        "        color = 'tab:blue'\n",
        "        ax2.set_ylabel('Temperature', color=color, rotation=270, labelpad=15)\n",
        "        ax2.plot(new_var['temp'], color=color, label=\"Temperature\")\n",
        "        ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        ax1.set_title(\"CartPole-v1 - Training\")\n",
        "\n",
        "        textstr = f\"\"\"replay\\_capacity: {new_var['replay_memory_capacity']}\n",
        "gamma: {new_var['gamma']}\n",
        "lr: {new_var['learning_rate']}\n",
        "batch: {new_var['batch_size']}\n",
        "update\\_every: {new_var['target_net_update_steps']}\n",
        "init\\_temp: {new_var['initial_temperature']}\n",
        "zero\\_temp\\_at: {new_var['reach_zero_temperature_after_n_episodes']}\"\"\"\n",
        "\n",
        "        props = dict(boxstyle='round, pad=.3', facecolor='gray', alpha=.1)\n",
        "        ax1.text(0.15, 0.95, textstr, transform=ax1.transAxes, fontsize=10, verticalalignment='top', bbox=props)\n",
        "        fig.tight_layout()\n",
        "\n",
        "        ax1.legend(loc=(0.45, 0.7))\n",
        "        ax2.legend(loc=(0.45, 0.9))\n",
        "\n",
        "        filename = os.path.splitext(os.path.basename(file))[0]\n",
        "        fig.savefig(f\"Plots/1/{filename}.pdf\", transparent=True, bbox_inches='tight')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca5345de3604402cb0b1259ccbd25197"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "710fff9638db4677ac690d5db652133e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPISODE 1 - FINAL SCORE: 500.0\n",
            "EPISODE 2 - FINAL SCORE: 500.0\n",
            "EPISODE 3 - FINAL SCORE: 500.0\n",
            "EPISODE 4 - FINAL SCORE: 500.0\n",
            "EPISODE 5 - FINAL SCORE: 500.0\n",
            "EPISODE 6 - FINAL SCORE: 500.0\n",
            "EPISODE 7 - FINAL SCORE: 500.0\n",
            "EPISODE 8 - FINAL SCORE: 500.0\n",
            "EPISODE 9 - FINAL SCORE: 500.0\n",
            "EPISODE 10 - FINAL SCORE: 500.0\n"
          ]
        }
      ],
      "source": [
        "from model import DeepQLearner\n",
        "from utils import wrap_env\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#---Test the final training---#\n",
        "\n",
        "model = DeepQLearner.load_from_checkpoint(\"SavedModels/24_06_21-00h43m46.ckpt\", min_samples_for_training=0) \n",
        "\n",
        "#NOTE: I have modified the DeepQLearner adding other parameters\n",
        "#over time, so loading different checkpoints may not work\n",
        "#due to some breaking changes in their interface (the architecuture is the same though)\n",
        "\n",
        "# Initialize the Gym environment\n",
        "env = gym.make('CartPole-v1') \n",
        "env.seed(1) # Set a random seed for the environment (reproducible results)\n",
        "\n",
        "# This is for creating the output video in Colab, not required outside Colab\n",
        "env = wrap_env(env, video_callable=lambda episode_id: True) # Save a video every episode\n",
        "\n",
        "model.eval()\n",
        "# Let's try for a total of 10 episodes\n",
        "for num_episode in tqdm(range(10)): \n",
        "    # Reset the environment and get the initial state\n",
        "    state = env.reset()\n",
        "    # Reset the score. The final score will be the total amount of steps before the pole falls\n",
        "    score = 0\n",
        "    done = False\n",
        "    # Go on until the pole falls off or the score reach 490\n",
        "    while not done:\n",
        "      with torch.no_grad():\n",
        "        action = int(model.policy_net(torch.tensor(state, dtype=torch.float32)).argmax())\n",
        "\n",
        "      # Apply the action and get the next state, the reward and a flag \"done\" that is True if the game is ended\n",
        "      next_state, reward, done, info = env.step(action)\n",
        "      # Visually render the environment\n",
        "      env.render()\n",
        "      # Update the final score (+1 for each step)\n",
        "      score += reward \n",
        "      # Set the current state for the next iteration\n",
        "      state = next_state\n",
        "      # Check if the episode ended (the pole fell down)\n",
        "    # Print the final score\n",
        "    print(f\"EPISODE {num_episode + 1} - FINAL SCORE: {score}\") \n",
        "env.close() "
      ]
    }
  ]
}